Subsection: Overall workflow  
The workflow illustrated in Figure <workflow> was applied to each of the five genes. Two regions of the dataset were considered: one with all wavebands (350-2500 nm) as predictors, and one with visible light only (400-700 nm). The dataset was then split 80%/20% into training and testing sets, respectively. Each predictor and target variable was standardized according to the training set’s distribution to have a mean of 0 and standard deviation of 1, allowing for meaningful comparison among wavebands and gene levels.
	We chose six waveband (feature) selection methods to consider, incorporating several paradigms. Mutual information and hierarchical clustering were both filter methods, using the linear model coefficients was an embedded method, and permutation importance and the genetic algorithms fit the wrapper paradigm.  These five methods chose important wavebands from the entire region of consideration (either all wavebands or visible light only). Then, a second genetic algorithm selected a consensus of the top wavebands from the concatenated results of the previous five methods. 
	After each of the six methods produced a subset of wavebands deemed most important for predictions, we trained a new elastic net model on the training set, but only using that subset of features. This was intended as a validation test; for example, if a method’s feature choices were poor, the validation model’s error would be very high. In addition to the validation models, a baseline model using no feature selection methods was trained for each gene and region   as a basis for comparison. The elastic net algorithm was chosen based on the results of <Brooks2024>, in which its predictions outperformed the random forest and LASSO algorithms on this dataset. Throughout the workflow, 5-fold grid search cross-validation was used within the training set for tuning the regularization penalty and ℓ1/ ℓ2 ratio hyperparameters. Each model was evaluated on the testing set based on the root mean square error (RMSE), mean absolute error (MAE), and R2 metrics.
 <TODO: add sentence here about how the targets were standardized, so SD = 1 = RMSE = MAE of blind model> Because the target variables were standardized, the performance of models predicting different genes could be compared on the same scale. Additionally, a “blind” model always predicting the mean value would be guaranteed a RMSE and MAE of approximately  1, the standard deviation. This provided a quantitative basis of comparison to evaluate model bias.
	We implemented this workflow in Python 3.12.7 using the scikit-learn 1.5.2 <cite scikit-learn> and sklearn-genetic-opt 0.11.1 <cite genetic> libraries. 
Subsection: Filter methods
	One feature selection method used in this study used mutual information as a metric. Mutual information, equivalent to information gain in this context, quantifies the dependence of one variable given another. The mutual information of each waveband with the gene level  was calculated, and the function returned the 64 wavebands with the largest MI values.
Our clustering method for waveband selection first calculated the Pearson correlation matrix for the predictors (wavebands). We then applied agglomerative clustering to the correlation matrix, using a distance threshold of 0.999 and Ward linkage. This threshold was chosen based on exploratory data analysis conducted in <brooks2024>, which demonstrated the high correlation of adjacent wavebands across the spectrum. After the clusters of wavebands were generated, one waveband was randomly chosen from each cluster. The set of these representative wavebands was returned as the clustering method’s selections. Because the clustering method only considered the similarity among wavebands, not taking the target variables into account, it only needed to be executed twice: once for the full dataset, and once for the visible-light-only version of the dataset .
Subsection: Embedded method
	The main reason that elastic net models were chosen in <brooks2024> was their inherent support for embedded feature selection. Because elastic net models are just variants of linear regression models, each input variable is assigned a weight. The absolute values of 
	The fundamental task of linear regression models is to find the set of weights w that minimizes the error (or variance) of the equation y=mX+b over all data points. Several  variants exist to minimize the unwanted effect of outliers in the dataset; one robust variant is elastic net regression, which finds the optimal values for w in the following expression:
\frac{1}{2n_{samples}}\left|\left|Xw-y\right|\right|_2^2+\alpha\rho\left|\left|w\right|\right|_1+\frac{\alpha\left(1-\rho\right)}{2}\left|\left|w\right|\right|_2^2 
where α is the regularization penalty, and ρ is the ratio between the ℓ1 and ℓ2  norms.
	In <brooks2024>, one reason that elastic net was chosen for investigation was its inherent support of an embedded feature selection method, namely, using its coefficients. A variable with a large positive or negative coefficient will have a stronger effect on predictions than one with a coefficient close to zero. When using this method, our function returned the set of 64 wavebands with the highest absolute valued coefficients.
Subsection: Wrapper methods
	Permutation importance  is another method of determining the magnitude of effect a variable has on a model’s predictions. After training and testing a model on the data, a single variable is permuted over the number of observations. A new model is then trained on the modified dataset. If the subsequent performance metrics are worse than the first model’s, this indicates that the permuted variable was important to the model’s predictions. A greater decrease in prediction quality indicates a higher importance for the permuted variable. This process is repeated for all variables in the dataset. To run permutation importance variable selection, we first applied agglomerative clustering <Add cross reference to clustering> among the variables, which was intended to reduce overall correlation among features. After calculating permutation importance for all variables, the function selected the 64 wavebands with the highest scores.
	The final method tested in this study used a genetic algorithm, implemented in the sklearn-genetic-opt Python library <cite genetic>, to select wavebands. A subset of variables was selected and iteratively improved according to an evolutionary algorithm. The effectiveness of each variable set was evaluated using 5-fold cross-validation on an elastic net model. The selection process minimized both the cross-validation RMSE and the number of features selected, with a maximum of 64 wavebands. When the genetic algorithm was used an additional time to find a consensus of the other waveband selections (see Figure <workflow>), a maximum of 16 wavebands were selected.
Results and Discussion
Subsection: Validation model metrics 
As a caveat when interpreting validation model metrics: assuming a model has not overfit, reducing the number of variables will reduce the quality of prediction. In this study, different waveband selection methods produced different numbers of features; models with fewer features tended to have higher RMSE, and vice versa. As the number of features was manually set as a hyperparameter for some methods, this could be adjusted. The key results are the wavebands deemed important for each gene, provided that method’s error was reasonably similar to the baseline. 
As <cross reference method_gene_rmse> suggests, a validation model’s RMSE has more to do with  the gene being predicted than the waveband selection method. For the same gene, different methods tended to have similar metrics. No waveband selection method resulted in a definitively superior or inferior RMSE for all five genes in this study. 16S rRNA and 18S rRNA are the more relevant genes in this dataset for monitoring soil health .
Elastic net models predicting 16S rRNA conclusively had the best results; the baseline model (using no waveband selection methods) using visible light wavebands only had the best metrics out of all models tested (RMSE = 0.724, MAE = 0.576, R2 = 0.501), outperforming the baseline model using all wavebands (RMSE = 0.762, MAE = 0.624, R2 = 0.448). This result not only demonstrates the feasibility of predicting 16S rRNA using hyperspectral data, but the better performance of the visible-light-only model also suggests the possibility of using more accessible sensors – hyperspectral sensors with a narrower range, multispectral sensors, or visible light sensors – in the future.
18S rRNA, on the other hand, was the most difficult gene to predict, with all validation models having RMSE exceeding 1, the standard deviation in the standardized set of gene levels. However, all models’ MAE scores were less than 1, with the best model selecting wavebands based on mutual information from the entire dataset (RMSE = 1.070, MAE = 0.860, R2 = 0.056). Although the difference between RMSE and MAE seems fairly large, it was typical for all genes’ validation models, suggesting 18S rRNA does not have an unusually large number of outliers compared to the other genes, and that the poor prediction quality could be due to the lack of a signal in the data.
Validation models predicting cbbLR, phoA, and ureC generally gave RMSE only slightly less than 1. This result is not as conclusive as the results of the other two genes, as this small improvement in prediction accuracy over a “blind” model does not suggest the presence of either a strong or weak signal in the hyperspectral data. 
Subsection: Waveband selections
Even though the hyperspectral data had a spectral resolution of 1 nm, waveband selection at this level of precision has questionable generalizability. Wavebands tended to have very large Pearson correlations with neighboring wavebands for very large regions of the spectrum, particularly in the near infrared (NIR)  region <cross reference to corr figure>. With correlation greater than 0.999 for adjacent wavebands across the spectrum, such wavebands become practically interchangeable. At this level, any variation in waveband selections might be due more to noise or stochastic algorithm choices than a true signal in the data. For this reason, we report waveband selection results at a coarser resolution, rounding to the nearest 10 nm.
	Generally, there was not strong agreement for important wavebands among regions and genes, but a few patterns emerged.  Broadly speaking, there was a preference for visible light when building models on all wavebands, and there was a preference for green, yellow, and orange light when building models on visible light wavebands only. Figures <consensus_histogram> and <consensus_histogram_gene> show the relatively wide dispersal of wavebands selected by consensus methods over each region. When allowing models to select from all wavebands, small clusters at 510-560 (green), 700-720 (RE ), and 970-980 (NIR ) were each chosen by multiple genes’ consensus models. The cluster from 1890-1940 (FSWIR ) was almost entirely chosen by the 16S rRNA consensus model, whose validation predictions had among the best metrics out of all tested models.
