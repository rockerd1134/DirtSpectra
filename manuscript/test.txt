The workflow illustrated in Figure <workflow> was applied to each of the five genes. Two versions of the dataset were considered: one with all wavebands (350-2500 nm) as predictors, and one with visible light only (400-700 nm). The dataset was then split 80%/20% into training and testing sets, respectively. Each variable was standardized according to the training set’s distribution to have a mean of 0 and standard deviation of 1, allowing for meaningful comparison among wavebands and gene levels.
	We chose six waveband (feature) selection methods to consider, incorporating several paradigms. Mutual information and hierarchical clustering were both filter methods, using the linear model coefficients was an embedded method, and permutation importance and the genetic algorithms fit the wrapper paradigm.  These five methods chose important wavebands from the entire region of consideration (either all wavebands or visible light only). Then, a second genetic algorithm selected a consensus of the top wavebands from the concatenated results of the previous five methods. 
	After each of the six methods produced a subset of wavebands deemed most important for predictions, we trained a new elastic net model on the training set, but only using that subset of features. This was intended as a validation test; for example, if a method’s feature choices were poor, the validation model’s error would be very high. The elastic net algorithm was chosen based on the results of <Brooks2024>, in which its predictions outperformed the random forest and LASSO algorithms on this dataset. Throughout the workflow, 5-fold grid search cross-validation was used within the training set for tuning the regularization penalty and ℓ1/ ℓ2 ratio hyperparameters. Each validation model was evaluated on the testing set based on the root mean square error (RMSE), mean absolute error (MAE), and R2 metrics. <TODO: add sentence here about how the targets were standardized, so SD = 1 = RMSE = MAE of blind model>
	We implemented this workflow in Python 3.12.7 using the scikit-learn 1.5.2 <cite scikit-learn> and sklearn-genetic-opt 0.11.1 <cite genetic> libraries. 
Subsection: Filter methods
	One feature selection method used in this study used mutual information as a metric. Mutual information, equivalent to information gain in this context, quantifies the dependence of one variable given another. The mutual information of each waveband with the gene level  was calculated, and the function returned the 64 wavebands with the largest MI values.
Our clustering method for waveband selection first calculated the Pearson correlation matrix for the predictors (wavebands). We then applied agglomerative clustering to the correlation matrix, using a distance threshold of 0.999 and Ward linkage. This threshold was chosen based on exploratory data analysis conducted in <brooks2024>, which demonstrated the high correlation of adjacent wavebands across the spectrum. After the clusters of wavebands were generated, one waveband was randomly chosen from each cluster. The set of these representative wavebands was returned as the clustering method’s selections. Because the clustering method only considered the similarity among wavebands, not taking the target variables into account, it only needed to be executed twice: once for the full dataset, and once for the visible-light-only version of the dataset .
Subsection: Embedded method
	The main reason that elastic net models were chosen in <brooks2024> was their inherent support for embedded feature selection. Because elastic net models are just variants of linear regression models, each input variable is assigned a weight. The absolute values of 
	The fundamental task of linear regression models is to find the set of weights w that minimizes the error (or variance) of the equation y=mX+b over all data points. Several  variants exist to minimize the unwanted effect of outliers in the dataset; one robust variant is elastic net regression, which finds the optimal values for w in the following expression:
\frac{1}{2n_{samples}}\left|\left|Xw-y\right|\right|_2^2+\alpha\rho\left|\left|w\right|\right|_1+\frac{\alpha\left(1-\rho\right)}{2}\left|\left|w\right|\right|_2^2 
where α is the regularization penalty, and ρ is the ratio between the ℓ1 and ℓ2  norms.
	In <brooks2024>, one reason that elastic net was chosen for investigation was its inherent support of an embedded feature selection method, namely, using its coefficients. A variable with a large positive or negative coefficient will have a stronger effect on predictions than one with a coefficient close to zero. When using this method, our function returned the set of 64 wavebands with the highest absolute valued coefficients.
Subsection: Wrapper methods
	Permutation importance  is another method of determining the magnitude of effect a variable has on a model’s predictions. After training and testing a model on the data, a single variable is permuted over the number of observations. A new model is then trained on the modified dataset. If the subsequent performance metrics are worse than the first model’s, this indicates that the permuted variable was important to the model’s predictions. A greater decrease in prediction quality indicates a higher importance for the permuted variable. This process is repeated for all variables in the dataset. To run permutation importance variable selection, we first applied agglomerative clustering <Add cross reference to clustering> among the variables, which was intended to reduce overall correlation among features. After calculating permutation importance for all variables, the function selected the 64 wavebands with the highest scores.
Subsubsection: Genetic algorithm
	The final method tested in this study used a genetic algorithm, implemented in the sklearn-genetic-opt Python library <cite genetic>, to select wavebands. A subset of variables was selected and iteratively improved according to an evolutionary algorithm. The effectiveness of each variable set was evaluated using 5-fold cross-validation on an elastic net model. The selection process minimized both the cross-validation RMSE and the number of features selected, with a maximum of 64 wavebands. When the genetic algorithm was used an additional time to find a consensus of the other waveband selections (see Figure <workflow>), a maximum of 16 wavebands were selected.
