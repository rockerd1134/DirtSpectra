{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ddf0f0f-59ca-491a-ba1a-9696d530bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn_genetic import GAFeatureSelectionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae4377e-5b4d-4703-ba10-075149c87331",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = git.Repo('.', search_parent_directories = True)\n",
    "root = repo.working_tree_dir\n",
    "\n",
    "SEED = 0\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# Intended for reproducible GA steps\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24de72-57da-4ed6-8c32-06f5c5079f4c",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ea6ca7-8a1a-4d9d-a4a6-d9ad89b66198",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_consol = pd.read_csv(root + '//data/data_consol.csv')\n",
    "\n",
    "X = data_consol.filter(regex=\"^[0-9]+$\")\n",
    "    \n",
    "# Note: do NOT scale X and y before splitting, since that is a data leak. Instead, use the pipeline to scale both Xs, and separately scale the y for custom scoring like RMSE.\n",
    "X_all_train, X_all_test, y_train_unscaled, y_test_unscaled = train_test_split(data_consol.filter(regex=\"^[0-9]+$\").to_numpy(), data_consol.filter(regex=\"pcr_[a-z]+_log\"), train_size=0.8, random_state=0)\n",
    "\n",
    "# Separate y by genes. Reshaping necessary for the y scaling step\n",
    "bact_train_unscaled = y_train_unscaled[\"pcr_bact_log\"].to_numpy().reshape(-1,1)\n",
    "bact_test_unscaled = y_test_unscaled[\"pcr_bact_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "cbblr_train_unscaled = y_train_unscaled[\"pcr_cbblr_log\"].to_numpy().reshape(-1,1)\n",
    "cbblr_test_unscaled = y_test_unscaled[\"pcr_cbblr_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "fungi_train_unscaled = y_train_unscaled[\"pcr_fungi_log\"].to_numpy().reshape(-1,1)\n",
    "fungi_test_unscaled = y_test_unscaled[\"pcr_fungi_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "urec_train_unscaled = y_train_unscaled[\"pcr_urec_log\"].to_numpy().reshape(-1,1)\n",
    "urec_test_unscaled = y_test_unscaled[\"pcr_urec_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "# Special case: phoa has 10 NAN rows that need to be removed from both its X and y.\n",
    "phoa_data = data_consol.filter(regex=\"^[0-9]+$|pcr_phoa_log\").dropna()\n",
    "X_all_phoa = phoa_data.to_numpy()[:,:2151]\n",
    "phoa = phoa_data[\"pcr_phoa_log\"].to_numpy()\n",
    "X_all_phoa_train, X_all_phoa_test, phoa_train_unscaled, phoa_test_unscaled = train_test_split(X_all_phoa, phoa, train_size=0.8, random_state=0)\n",
    "phoa_train_unscaled = phoa_train_unscaled.reshape(-1,1)\n",
    "phoa_test_unscaled = phoa_test_unscaled.reshape(-1,1)\n",
    "\n",
    "# Create copies of the X sets for visible light only (using bounds of 400 nm -> 700 nm)\n",
    "X_vis_train = X_all_train[:,400:701]\n",
    "X_vis_test = X_all_test[:,400:701]\n",
    "X_vis_phoa_train = X_all_phoa_train[:,400:701]\n",
    "X_vis_phoa_test = X_all_phoa_test[:,400:701]\n",
    "\n",
    "# Scale each y with respect to its distribution\n",
    "bact_scaler = StandardScaler()\n",
    "bact_train = bact_scaler.fit_transform(bact_train_unscaled).reshape(-1,1)\n",
    "bact_test = bact_scaler.transform(bact_test_unscaled).reshape(-1,1)\n",
    "\n",
    "cbblr_scaler = StandardScaler()\n",
    "cbblr_train = cbblr_scaler.fit_transform(cbblr_train_unscaled).reshape(-1,1)\n",
    "cbblr_test = cbblr_scaler.transform(cbblr_test_unscaled).reshape(-1,1)\n",
    "\n",
    "fungi_scaler = StandardScaler()\n",
    "fungi_train = fungi_scaler.fit_transform(fungi_train_unscaled).reshape(-1,1)\n",
    "fungi_test = fungi_scaler.transform(fungi_test_unscaled).reshape(-1,1)\n",
    "\n",
    "phoa_scaler = StandardScaler()\n",
    "phoa_train = phoa_scaler.fit_transform(phoa_train_unscaled).reshape(-1,1)\n",
    "phoa_test = phoa_scaler.transform(phoa_test_unscaled).reshape(-1,1)\n",
    "\n",
    "urec_scaler = StandardScaler()\n",
    "urec_train = urec_scaler.fit_transform(urec_train_unscaled).reshape(-1,1)\n",
    "urec_test = urec_scaler.transform(urec_test_unscaled).reshape(-1,1)\n",
    "\n",
    "# 5-fold CV; random state 0\n",
    "cv_5_0 = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Used for waveband selection\n",
    "wvs = np.arange(350,2501)\n",
    "wvs_vis = np.arange(400,701)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776dafe2-b561-42aa-8555-1ddb10ceaf27",
   "metadata": {},
   "source": [
    "**The major pipeline components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1baafb33-2874-480d-b8f5-b24da784c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet(fit_intercept=False, warm_start=True, random_state=0, selection='random', max_iter=8000)\n",
    "\n",
    "# Used for embedded feature importance (via coeffs) and wrapper feature importance (via perm importance)\n",
    "pipe_elastic_net = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"elastic_net\", elastic_net)\n",
    "    ],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Hyperparameters for elastic net tuning. When code is finalized, expand for more thorough search using more computational resources.\n",
    "REGULARIZATION = np.logspace(-5, 5, 16)\n",
    "MIXTURE = np.linspace(0.001, 1, 16)\n",
    "PARAM_GRID = [\n",
    "    {\n",
    "        \"elastic_net__alpha\": REGULARIZATION,\n",
    "        \"elastic_net__l1_ratio\": MIXTURE\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f709e-3d36-4b05-bd3c-8559363a2ec8",
   "metadata": {},
   "source": [
    "**The baseline models (for comparison)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289243c7-7803-45e4-b123-5c537c3fe981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(X_train, y_train, X_test):\n",
    "    \"\"\" Build an elastic net model on the whole set of wavebands considered (no waveband selection methods used). \n",
    "    Returns the predictions on y_train.\"\"\"\n",
    "    print('\\tStarting baseline...', end='')\n",
    "    model = GridSearchCV(estimator=pipe_elastic_net, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    print('Done.')\n",
    "    return preds, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4869b-ea67-43a0-91fc-589017e4b056",
   "metadata": {},
   "source": [
    "**The feature selection functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c0a9738-0784-43a0-8381-2c223d954501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is only with respect to X_train, not any of the target variables, this only has to be computed once. (It's relatively cheap to compute, but this also has the benefit of preserving the random choices.)\n",
    "def cluster(X_train, region):\n",
    "    \"\"\" Uses agglomerative clustering with a distance threshold of 0.999 on the normalized feature correlation coefficient matrix. Then, it randomly selects one waveband from each cluster.\n",
    "    This should be used as a preprocessing step when doing permutation importance. (Clustering method) \"\"\"\n",
    "    corr = np.corrcoef(X_train.T) # X needs to be transposed because of corrcoef's implementation\n",
    "    agg = AgglomerativeClustering(n_clusters=None, distance_threshold=0.999) # The distance threshold is somewhat arbitrary, but it's based on EDA and domain knowledge, and the results seem reasonable.\n",
    "    clusters = agg.fit_predict(corr)\n",
    "    # Now select a single \"representative\" waveband from each cluster\n",
    "    if(region == \"all\"):\n",
    "        wavebands = wvs\n",
    "    elif(region == \"vis\"):\n",
    "        wavebands = wvs_vis\n",
    "    cluster_choices = []\n",
    "    for i in range(np.max(clusters)):\n",
    "        wv_in_cluster = wavebands[clusters==i]\n",
    "        cluster_choices.append(rng.choice(wv_in_cluster))\n",
    "    cluster_choices = np.sort(np.array(cluster_choices))\n",
    "    return cluster_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57fa7c96-2030-4c86-81ca-4e3846bfbfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go ahead and call this once.\n",
    "cluster_choices_all = cluster(X_all_train, \"all\")\n",
    "cluster_choices_vis = cluster(X_vis_train, \"vis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9295e2ac-fa83-4f85-8584-b718b6bcb43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi(X_train, y_train, region, n_features=64):\n",
    "    \"\"\" Uses mutual information to calculate the n_features most related features in X_train to y_train. (Filter method) \"\"\"\n",
    "    y_train = y_train.ravel()\n",
    "    mi = mutual_info_regression(X_train, y_train)\n",
    "    top_n_idx = np.argpartition(mi, -n_features)[-n_features:]\n",
    "    if(region == \"all\"):\n",
    "        return wvs[top_n_idx]\n",
    "    elif(region == \"vis\"):\n",
    "        return wvs_vis[top_n_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c44df7d0-4b03-45a2-825f-53ca890709f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_elastic_net(X_train, y_train):\n",
    "    \"\"\" Builds and fits an elastic net model using all features. \n",
    "    Returns the fit estimator (a pipeline). Used within coeffs() and ga(). \"\"\"\n",
    "    grid = GridSearchCV(estimator=pipe_elastic_net, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "    grid.fit(X_train, y_train)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "509a8a71-046a-4d56-a1ae-62fb72f52607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeffs(estimator, region, n_features=64):\n",
    "    \"\"\" Builds and fits an elastic net model using all features. Returns the n_features features with the highest absolute-valued coefficients. (Embedded method) \"\"\"\n",
    "    coeffs = estimator['elastic_net'].coef_\n",
    "    abs_coeffs = np.abs(coeffs)\n",
    "    top_n_idx = np.argpartition(abs_coeffs, -n_features)[-n_features:]\n",
    "    if(region == \"all\"):\n",
    "        return wvs[top_n_idx]\n",
    "    elif(region == \"vis\"):\n",
    "        return wvs_vis[top_n_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9776accc-bcd2-4c54-a8cf-75586fec77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ga(X_train, y_train, trained_estimator, wv_subset, n_features=64):\n",
    "    \"\"\" Uses a genetic algorithm to find the wavebands that gives the lowest RMSE on an elastic net model. \n",
    "    The subset will be at most n_features large, but it may be less than n_features large. \n",
    "    wv_subset should be wvs (or wvs_vis) when in the feature selection layer, but when in the consolidation layer, it should\n",
    "    be the subset of possible wavelengths output by the concatenated feature selection methods, not the entire\n",
    "    [350,2500] (or [400,700]) set. (GA method) \"\"\"\n",
    "    \n",
    "    y_train = y_train.ravel()\n",
    "    ga_selector = GAFeatureSelectionCV(\n",
    "        estimator=trained_estimator,\n",
    "        cv=cv_5_0,  # Cross-validation folds\n",
    "        scoring=\"neg_root_mean_squared_error\",  # Fitness function (maximize accuracy)\n",
    "        population_size=n_features*2,  # Number of individuals in the population\n",
    "        generations=20,  # Number of generations\n",
    "        n_jobs=-1,  # Use all available CPU cores\n",
    "        verbose=False,\n",
    "        max_features=n_features,\n",
    "        return_train_score=True,\n",
    "        refit=False,\n",
    "        crossover_probability=0.8,\n",
    "        mutation_probability=0.2\n",
    "    )\n",
    "    pipe_ga = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"ga\", ga_selector)\n",
    "        ],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    pipe_ga.fit(X_train, y_train)\n",
    "    feats = pipe_ga['ga'].best_features_ # A mask of the features selected from X_train\n",
    "        \n",
    "    return wv_subset[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "423897b7-709f-487a-a061-4a6e63e532b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi(X_train, y_train, region, n_features=64):\n",
    "    \"\"\" Calculates permutation importance on a dataset. cluster_choices should be the result of calling cluster(), which should be done once at the start of execution. \n",
    "    This is done outside this function to preserve the random selection. Returns the set of n_features wavebands with the highest permutation importance on the training set. (Wrapper method) \"\"\"\n",
    "    # Use only the features selected by clustering\n",
    "    if(region == \"all\"):\n",
    "        cluster_choices = cluster_choices_all\n",
    "        cluster_idx = cluster_choices - 350\n",
    "    elif(region == \"vis\"):\n",
    "        cluster_choices = cluster_choices_vis\n",
    "        cluster_idx = cluster_choices - 400 # Needed since in vis, waveband 400 is the 0th.\n",
    "    \n",
    "    X_train = X_train[:,cluster_idx]\n",
    "    # Build and train another elastic net model, but only on the features left after clustering, to use for permutation importance.\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"elastic_net\", elastic_net)\n",
    "        ], \n",
    "        verbose=False\n",
    "    )    \n",
    "    grid = GridSearchCV(estimator=pipe, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "    grid.fit(X_train, y_train)\n",
    "    pi = permutation_importance(grid, X_train, y_train, scoring='neg_root_mean_squared_error', n_repeats=10, n_jobs=-1, random_state=0)\n",
    "    # Needed in case fewer than the threshold were chosen by the method\n",
    "    n = min(pi.importances_mean.shape[0], n_features)\n",
    "    pi_top_n_idx = np.argpartition(pi.importances_mean, -n)[-n:]\n",
    "    return cluster_choices[pi_top_n_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109eb5fc-0120-4f23-948a-c30c127abf26",
   "metadata": {},
   "source": [
    "**Consensus function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c8b10c4-d887-44f0-a8c2-2db135985c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus(X_train, y_train, region, n_features_intermed=64, max_features_output=16):\n",
    "    \"\"\" Takes the wavebands output by the feature selection functions and uses a (separate) genetic algorithm to find the wavebands that give the lowest RMSE on an elastic net model.\n",
    "    The subset will be at most n_features large, but it may be less than n_features large.\n",
    "    Returns the tuple: (wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_pi, wv_consensus), where each is a numpy array of wavebands that were selected by each method. \"\"\"\n",
    "    \n",
    "    if(region == \"all\"):\n",
    "        wv_subset = wvs # Used for GA\n",
    "        wv_cluster = cluster_choices_all # Used when appending to wv_intermed\n",
    "    elif(region == \"vis\"):\n",
    "        wv_subset = wvs_vis\n",
    "        wv_cluster = cluster_choices_vis\n",
    "    \n",
    "    print('\\tStarting mutual importance...', end=' ')\n",
    "    wv_mi = mi(X_train, y_train, region, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "    print('\\tTraining the elastic net model...', end=' ')\n",
    "    trained_pipe = train_elastic_net(X_train, y_train)\n",
    "    wv_coeffs = coeffs(trained_pipe, region, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "    print('\\tStarting genetic algorithm...', end=' ')\n",
    "    wv_ga = ga(X_train, y_train, trained_pipe, wv_subset, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "    print('\\tStarting permutation importance...', end=' ')\n",
    "    wv_pi = pi(X_train, y_train, region, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "\n",
    "    # Compile the above results into one array, remove any duplicates, and sort.\n",
    "    wv_intermed = np.append(wv_mi, wv_coeffs)\n",
    "    wv_intermed = np.append(wv_intermed, wv_ga)\n",
    "    wv_intermed = np.append(wv_intermed, wv_cluster)\n",
    "    wv_intermed = np.append(wv_intermed, wv_pi)\n",
    "    wv_intermed = np.sort(np.unique(wv_intermed))\n",
    "\n",
    "    # Convert the above into indices for masking over the dataset.\n",
    "    if(region == \"all\"):\n",
    "        wv_intermed_idx = wv_intermed-350\n",
    "    elif(region == \"vis\"):\n",
    "        wv_intermed_idx = wv_intermed-400\n",
    "    X_train = X_train[:,wv_intermed_idx]\n",
    "\n",
    "    # Use another genetic algorithm to find the best wavebands out of the narrowed possibilities\n",
    "    print('\\tStarting genetic algorithm...', end=' ')\n",
    "    wv_consensus = ga(X_train, y_train, trained_pipe, wv_intermed, n_features=max_features_output)\n",
    "    print('\\tDone.')\n",
    "    return (wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_pi, wv_consensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704d0d3-cdd5-43b3-a80a-5e26f3dd1ec8",
   "metadata": {},
   "source": [
    "**The \"main\" function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32facd2c-639d-4f71-812f-1d3b8649ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would normally be in the main function, but defined separately for easier testing, debugging, and analysis after running within a Jupyter notebook.\n",
    "def run():\n",
    "    # Lists of results that will be compiled at the end into a DataFrame for writing to CSV\n",
    "    region_list = [] # \"all\" or \"vis\"\n",
    "    gene_list = [] # \"bact\", \"fungi\", etc.\n",
    "    method_list = [] # \"mi\", \"coeffs\", etc.\n",
    "    wv_list = [] # The (int) wavebands selected/considered\n",
    "    coeff_list = [] # The fitted coefficients for each of the above wavebands (no intercept was calculated)\n",
    "    penalty_list = [] # The regularization penalty for the model\n",
    "    ratio_list = [] # The l1-l2 ratio for the model\n",
    "    rmse_list = [] # The RMSE score for the model\n",
    "    r2_list = [] # The R2 score for the model\n",
    "    mae_list = [] # The MAE score for the model\n",
    "\n",
    "    # Loop over both the whole waveband range and visible light only. (Making the loop in this format in case there are any other regions we want to add later.)\n",
    "    # Swapped order to vis first, then all, for fail-fast testing\n",
    "    for starting_region in (\"vis\", \"all\"):\n",
    "\n",
    "        if(starting_region == \"all\"):\n",
    "            X_train = X_all_train\n",
    "            X_test = X_all_test\n",
    "            X_phoa_train = X_all_phoa_train\n",
    "            X_phoa_test = X_all_phoa_test\n",
    "            region_set = wvs\n",
    "        elif(starting_region == \"vis\"):\n",
    "            X_train = X_vis_train\n",
    "            X_test = X_vis_test\n",
    "            X_phoa_train = X_vis_phoa_train\n",
    "            X_phoa_test = X_vis_phoa_test\n",
    "            region_set = wvs_vis\n",
    "            \n",
    "        # Loop over each gene (y value)\n",
    "        for gene, y_train, y_test in zip((\"phoa\", \"cbblr\", \"fungi\", \"bact\", \"urec\"), (phoa_train, cbblr_train, fungi_train, bact_train, urec_train), (phoa_test, cbblr_test, fungi_test, bact_test, urec_test)):\n",
    "    \n",
    "            print(\"Starting \", gene, \"...\", sep = \"\")\n",
    "\n",
    "            # Build a baseline model on the entire region of consideration (no waveband selection methods used) for comparison, and record results\n",
    "            if(gene == \"phoa\"):\n",
    "                baseline_preds, baseline_model = baseline(X_phoa_train, y_train, X_phoa_test)\n",
    "            else:\n",
    "                baseline_preds, baseline_model = baseline(X_train, y_train, X_test)\n",
    "            model = baseline_model.best_estimator_['elastic_net']\n",
    "            penalty = model.alpha\n",
    "            ratio = model.l1_ratio\n",
    "            coeffs = model.coef_\n",
    "            rmse = root_mean_squared_error(y_test, baseline_preds) * -1\n",
    "            r2 = r2_score(y_test, baseline_preds)\n",
    "            mae = mean_absolute_error(y_test, baseline_preds)\n",
    "\n",
    "            # Record each waveband/coeff separately, in tidy format for easier analysis\n",
    "            for i, wv in enumerate(region_set):\n",
    "                region_list.append(starting_region)\n",
    "                gene_list.append(gene)\n",
    "                method_list.append(\"baseline\")\n",
    "                wv_list.append(wv)\n",
    "                coeff_list.append(coeffs[i])\n",
    "                penalty_list.append(penalty)\n",
    "                ratio_list.append(ratio)\n",
    "                rmse_list.append(rmse)\n",
    "                r2_list.append(r2)\n",
    "                mae_list.append(mae)\n",
    "            # Pickle the model\n",
    "            model_path = root + '//cache/' + starting_region + \"_\" + gene + \"_baseline.pickle\"\n",
    "            with open(model_path, 'wb') as file:\n",
    "                pickle.dump(model, file)\n",
    "\n",
    "            # Where the main calculations happen. Runs each method separately, and then finds the consensus of all of them.\n",
    "            # The phoa special case is due to a different train/test split than the rest because of some NANs.\n",
    "            if(gene == \"phoa\"):\n",
    "                wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_pi, wv_consensus = consensus(X_phoa_train, y_train, starting_region)\n",
    "            else:\n",
    "                wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_pi, wv_consensus = consensus(X_train, y_train, starting_region)\n",
    "            \n",
    "            for method, wv_set in zip((\"mi\", \"coeffs\", \"ga\", \"cluster\", \"pi\", \"consensus\"), (wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_pi, wv_consensus)):\n",
    "                \n",
    "                # Build a new elastic net model for validation on this subset of wavebands\n",
    "                if(starting_region == \"all\"):\n",
    "                    wv_set_idx = wv_set-350\n",
    "                elif(starting_region == \"vis\"):\n",
    "                    wv_set_idx = wv_set-400\n",
    "                validator = GridSearchCV(estimator=pipe_elastic_net, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "                # Like above, special case for phoa\n",
    "                if(gene == \"phoa\"):\n",
    "                    validator.fit(X_phoa_train[:,wv_set_idx], y_train)\n",
    "                    preds = validator.predict(X_phoa_test[:,wv_set_idx])\n",
    "                else:\n",
    "                    validator.fit(X_train[:,wv_set_idx], y_train)\n",
    "                    preds = validator.predict(X_test[:,wv_set_idx])\n",
    "                model = validator.best_estimator_['elastic_net']\n",
    "                penalty = model.alpha\n",
    "                ratio = model.l1_ratio\n",
    "                coeffs = model.coef_\n",
    "                rmse = root_mean_squared_error(y_test, preds) * -1\n",
    "                r2 = r2_score(y_test, preds)\n",
    "                mae = mean_absolute_error(y_test, preds)\n",
    "\n",
    "                for i, wv in enumerate(wv_set):\n",
    "                    region_list.append(starting_region)\n",
    "                    gene_list.append(gene)\n",
    "                    method_list.append(method)\n",
    "                    wv_list.append(wv)\n",
    "                    coeff_list.append(coeffs[i])\n",
    "                    penalty_list.append(penalty)\n",
    "                    ratio_list.append(ratio)\n",
    "                    rmse_list.append(rmse)\n",
    "                    r2_list.append(r2)\n",
    "                    mae_list.append(mae)\n",
    "                model_path = root + '//cache/' + starting_region + \"_\" + gene + \"_\" + method + \".pickle\"\n",
    "                with open(model_path, 'wb') as file:\n",
    "                    pickle.dump(model, file)\n",
    "                    \n",
    "            print(\"Finished \", gene, \".\", sep = \"\")\n",
    "\n",
    "    # Compile the results into a single DataFrame and write it to a CSV\n",
    "    # The format will make for easier analysis later.\n",
    "    print(\"Compiling and writing results to CSV...\", end = \" \")\n",
    "    col_names = ['gene', 'method', 'wv', 'rmse']\n",
    "    results = pd.DataFrame(columns = col_names)\n",
    "    results['region'] = region_list\n",
    "    results['gene'] = gene_list\n",
    "    results['method'] = method_list\n",
    "    results['wv'] = wv_list\n",
    "    results['coeff'] = coeff_list\n",
    "    results['penalty'] = penalty_list\n",
    "    results['ratio'] = ratio_list\n",
    "    results['rmse'] = rmse_list\n",
    "    results['r2'] = r2_list\n",
    "    results['mae'] = mae_list\n",
    "    results.to_csv(root + '//results/results.csv', index=False)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "280fa014-0678-4a83-b315-87560aedc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
