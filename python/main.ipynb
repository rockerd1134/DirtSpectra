{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd91d8c-be40-4c4f-b55a-2aa13f2708f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn_genetic import GAFeatureSelectionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c35222-7cae-4d10-8aa1-93889b0117f6",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d6faae-0195-426f-90d5-db5b48f055dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether using data_consol.csv helps anything. If so, probably indicates an error in reading in or joining the separate CSVs before\n",
    "repo = git.Repo('.', search_parent_directories = True)\n",
    "root = repo.working_tree_dir\n",
    "\n",
    "data_consol = pd.read_csv(root + '//data/data_consol.csv')\n",
    "\n",
    "SEED = 0\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# Intended for reproducible GA steps\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "X = data_consol.filter(regex=\"^[0-9]+$\")\n",
    "    \n",
    "# Note: do NOT scale X and y before splitting, since that is a data leak. Instead, use the pipeline to scale both Xs, and separately scale the y for custom scoring like RMSE.\n",
    "X_all_train, X_all_test, y_train_unscaled, y_test_unscaled = train_test_split(data_consol.filter(regex=\"^[0-9]+$\").to_numpy(), data_consol.filter(regex=\"pcr_[a-z]+_log\"), train_size=0.8, random_state=0)\n",
    "\n",
    "# Separate y by genes. Reshaping necessary for the y scaling step\n",
    "bact_train_unscaled = y_train_unscaled[\"pcr_bact_log\"].to_numpy().reshape(-1,1)\n",
    "bact_test_unscaled = y_test_unscaled[\"pcr_bact_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "cbblr_train_unscaled = y_train_unscaled[\"pcr_cbblr_log\"].to_numpy().reshape(-1,1)\n",
    "cbblr_test_unscaled = y_test_unscaled[\"pcr_cbblr_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "fungi_train_unscaled = y_train_unscaled[\"pcr_fungi_log\"].to_numpy().reshape(-1,1)\n",
    "fungi_test_unscaled = y_test_unscaled[\"pcr_fungi_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "urec_train_unscaled = y_train_unscaled[\"pcr_urec_log\"].to_numpy().reshape(-1,1)\n",
    "urec_test_unscaled = y_test_unscaled[\"pcr_urec_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "# Special case: phoa has 10 NAN rows that need to be removed from both its X and y.\n",
    "phoa_data = data_consol.filter(regex=\"^[0-9]+$|pcr_phoa_log\").dropna()\n",
    "X_all_phoa = phoa_data.to_numpy()[:,:2151]\n",
    "phoa = phoa_data[\"pcr_phoa_log\"].to_numpy()\n",
    "X_all_phoa_train, X_all_phoa_test, phoa_train_unscaled, phoa_test_unscaled = train_test_split(X_phoa, phoa, train_size=0.8, random_state=0)\n",
    "phoa_train_unscaled = phoa_train_unscaled.reshape(-1,1)\n",
    "phoa_test_unscaled = phoa_test_unscaled.reshape(-1,1)\n",
    "\n",
    "# Create copies of the X sets for visible light only (using bounds of 400 nm -> 700 nm)\n",
    "X_vis_train = X_all_train[:,400:701]\n",
    "X_vis_test = X_all_test[:,400:701]\n",
    "X_vis_phoa_train = X_all_phoa_train[:,400:701]\n",
    "X_vis_phoa_test = X_all_phoa_test[:,400:701]\n",
    "\n",
    "# Scale each y with respect to its distribution\n",
    "bact_scaler = StandardScaler()\n",
    "bact_train = bact_scaler.fit_transform(bact_train_unscaled).reshape(-1,1)\n",
    "bact_test = bact_scaler.transform(bact_test_unscaled).reshape(-1,1)\n",
    "\n",
    "cbblr_scaler = StandardScaler()\n",
    "cbblr_train = cbblr_scaler.fit_transform(cbblr_train_unscaled).reshape(-1,1)\n",
    "cbblr_test = cbblr_scaler.transform(cbblr_test_unscaled).reshape(-1,1)\n",
    "\n",
    "fungi_scaler = StandardScaler()\n",
    "fungi_train = fungi_scaler.fit_transform(fungi_train_unscaled).reshape(-1,1)\n",
    "fungi_test = fungi_scaler.transform(fungi_test_unscaled).reshape(-1,1)\n",
    "\n",
    "phoa_scaler = StandardScaler()\n",
    "phoa_train = phoa_scaler.fit_transform(phoa_train_unscaled).reshape(-1,1)\n",
    "phoa_test = phoa_scaler.transform(phoa_test_unscaled).reshape(-1,1)\n",
    "\n",
    "urec_scaler = StandardScaler()\n",
    "urec_train = urec_scaler.fit_transform(urec_train_unscaled).reshape(-1,1)\n",
    "urec_test = urec_scaler.transform(urec_test_unscaled).reshape(-1,1)\n",
    "\n",
    "# 5-fold CV; random state 0\n",
    "cv_5_0 = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Used for waveband selection\n",
    "wvs = np.arange(350,2501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92808522-e1a1-4167-842a-913c8fadcd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is only with respect to X_train, not any of the target variables, this only has to be computed once. (It's relatively cheap to compute, but this also has the benefit of preserving the random choices.)\n",
    "def cluster(X_train):\n",
    "    \"\"\" Uses agglomerative clustering with a distance threshold of 0.999 on the normalized feature correlation coefficient matrix. Then, it randomly selects one waveband from each cluster.\n",
    "    This should be used as a preprocessing step when doing permutation importance. (Clustering method) \"\"\"\n",
    "    corr = np.corrcoef(X.T) # X needs to be transposed because of corrcoef's implementation\n",
    "    agg = AgglomerativeClustering(n_clusters=None, distance_threshold=0.999) # The distance threshold is somewhat arbitrary, but it's based on EDA and domain knowledge, and the results seem reasonable.\n",
    "    clusters = agg.fit_predict(corr)\n",
    "    # Now select a single \"representative\" waveband from each cluster\n",
    "    cluster_choices = []\n",
    "    for i in range(np.max(clusters)):\n",
    "        wv_in_cluster = wvs[clusters==i]\n",
    "        cluster_choices.append(rng.choice(wv_in_cluster))\n",
    "    cluster_choices = np.sort(np.array(cluster_choices))\n",
    "    return cluster_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ac874-fce3-4b88-b42e-00897bae084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_choices = cluster(X_all_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80f2e2",
   "metadata": {},
   "source": [
    "**The major pipeline components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e4672-3926-402d-b16c-85ec2caa98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet(fit_intercept=False, warm_start=True, random_state=0, selection='random', max_iter=4000)\n",
    "\n",
    "# Used for embedded feature importance (via coeffs) and wrapper feature importance (via perm importance)\n",
    "pipe_elastic_net = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"elastic_net\", elastic_net)\n",
    "    ],\n",
    "    memory = root+'\\\\cache',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Hyperparameters for elastic net tuning. When code is finalized, expand for more thorough search using more computational resources.\n",
    "REGULARIZATION = np.logspace(-5, 0, 8)\n",
    "MIXTURE = np.linspace(0.001, 1, 8)\n",
    "PARAM_GRID = [\n",
    "    {\n",
    "        \"elastic_net__alpha\": REGULARIZATION,\n",
    "        \"elastic_net__l1_ratio\": MIXTURE\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d64d3-904a-4f44-a923-cb8ce27dd59a",
   "metadata": {},
   "source": [
    "**Feature selection functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09dedf-8f50-4c85-951d-c911328949bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi(X_train, y_train, n_features=64):\n",
    "    \"\"\" Uses mutual information to calculate the n_features most related features in X_train to y_train. (Filter method) \"\"\"\n",
    "    y_train = y_train.ravel()\n",
    "    mi = mutual_info_regression(X_train, y_train)\n",
    "    top_n_idx = np.argpartition(mi, -n_features)[-n_features:]\n",
    "    return wvs[top_n_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c05613-121e-4666-ba40-c285cf7c4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_elastic_net(X_train, y_train):\n",
    "    \"\"\" Builds and fits an elastic net model using all features. \n",
    "    Returns the fit estimator (a pipeline). Used within coeffs() and ga(). \"\"\"\n",
    "    grid = GridSearchCV(estimator=pipe_elastic_net, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "    grid.fit(X_train, y_train)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e5f4d2-3b6f-433f-92df-b2c8279ba4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeffs(estimator, n_features=64):\n",
    "    \"\"\" Builds and fits an elastic net model using all features. Returns the n_features features with the highest absolute-valued coefficients. (Embedded method) \"\"\"\n",
    "    coeffs = estimator['elastic_net'].coef_\n",
    "    abs_coeffs = np.abs(coeffs)\n",
    "    top_n_idx = np.argpartition(abs_coeffs, -n_features)[-n_features:]\n",
    "    return wvs[top_n_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693601be-723e-4ce0-b1a5-676fbdd59413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ga(X_train, y_train, trained_estimator, n_features=64, wv_subset=None):\n",
    "    \"\"\" Uses a genetic algorithm to find the wavebands that gives the lowest RMSE on an elastic net model. \n",
    "    The subset will be at most n_features large, but it may be less than n_features large. \n",
    "    wv_subset should be None when in the feature selection layer, but when in the consolidation layer, it should\n",
    "    be the subset of possible wavelengths output by the concatenated feature selection methods, not the entire\n",
    "    [350,2500] set. (GA method) \"\"\"\n",
    "    \n",
    "    y_train = y_train.ravel()\n",
    "    ga_selector = GAFeatureSelectionCV(\n",
    "        estimator=trained_estimator,\n",
    "        cv=cv_5_0,  # Cross-validation folds\n",
    "        scoring=\"neg_root_mean_squared_error\",  # Fitness function (maximize accuracy)\n",
    "        population_size=n_features*2,  # Number of individuals in the population\n",
    "        generations=20,  # Number of generations\n",
    "        n_jobs=-1,  # Use all available CPU cores\n",
    "        verbose=False,\n",
    "        max_features=n_features,\n",
    "        return_train_score=True,\n",
    "        refit=False,\n",
    "        crossover_probability=0.8,\n",
    "        mutation_probability=0.2\n",
    "    )\n",
    "    pipe_ga = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"ga\", ga_selector)\n",
    "        ], \n",
    "        memory = root+'\\\\cache',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    pipe_ga.fit(X_train, y_train)\n",
    "    feats = pipe_ga['ga'].best_features_ # A mask of the features selected from X_train\n",
    "\n",
    "    # Should be the case in the feature selection layer\n",
    "    if wv_subset is None:\n",
    "        return wvs[feats]\n",
    "    # Should be the case in the consensus layer\n",
    "    else:\n",
    "        return wv_subset[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78bd175-1cf6-4533-a305-dacce062f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_imp(X_train, y_train, n_features=64):\n",
    "    \"\"\" Calculates permutation importance on a dataset. cluster_choices should be the result of calling cluster(), which should be done once at the start of execution. \n",
    "    This is done outside this function to preserve the random selection. Returns the set of n_features wavebands with the highest permutation importance on the training set. (Wrapper method) \"\"\"\n",
    "    # Use only the features selected by clustering\n",
    "    cluster_idx = cluster_choices - 350\n",
    "    X_train = X_train[:,cluster_idx]\n",
    "    # Build and train another elastic net model, but only on the features left after clustering, to use for permutation importance.\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"elastic_net\", elastic_net)\n",
    "        ], \n",
    "        memory = root+'\\\\cache',\n",
    "        verbose=False\n",
    "    )    \n",
    "    grid = GridSearchCV(estimator=pipe, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "    grid.fit(X_train, y_train)\n",
    "    perm_imp = permutation_importance(grid, X_train, y_train, scoring='neg_root_mean_squared_error', n_repeats=10, n_jobs=-1, random_state=0)\n",
    "    pi_top_64_idx = np.argpartition(perm_imp.importances_mean, -64)[-64:]\n",
    "    return cluster_choices[pi_top_64_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465453b6-7b18-40c6-b88a-502d1a9f4c9e",
   "metadata": {},
   "source": [
    "**Consensus function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed9401-293e-4afd-82e3-45e57f1bccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus(X_train, y_train, n_features_intermed=64, max_features_output=8):\n",
    "    \"\"\" Takes the wavebands output by the feature selection functions and uses a (separate) genetic algorithm to find the wavebands that give the lowest RMSE on an elastic net model.\n",
    "    The subset will be at most n_features large, but it may be less than n_features large.\n",
    "    Returns the tuple: (wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus), where each is a numpy array of wavebands that were selected by each method.  (Consensus method) \"\"\"\n",
    "    \n",
    "    print('\\tFEATURE SELECTION:')\n",
    "    print('\\tStarting mutual importance...', end=' ')\n",
    "    wv_mi = mi(X_train, y_train, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "    print('\\tTraining the elastic net model...', end=' ')\n",
    "    trained_pipe = train_elastic_net(X_train, y_train)\n",
    "    wv_coeffs = coeffs(trained_pipe, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "    print('\\tStarting genetic algorithm...', end=' ')\n",
    "    wv_ga = ga(X_train, y_train, trained_pipe, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "    print('\\tStarting permutation importance...', end=' ')\n",
    "    wv_cluster = cluster_choices # Doesn't require a separate function call\n",
    "    wv_perm_imp = perm_imp(X_train, y_train, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "\n",
    "    # Compile the above results into one array, remove any duplicates, and sort.\n",
    "    wv_intermed = np.append(wv_mi, wv_coeffs)\n",
    "    wv_intermed = np.append(wv_intermed, wv_ga)\n",
    "    wv_intermed = np.append(wv_intermed, wv_cluster)\n",
    "    wv_intermed = np.append(wv_intermed, wv_perm_imp)\n",
    "    wv_intermed = np.sort(np.unique(wv_intermed))\n",
    "\n",
    "    # Convert the above into indices for masking over the dataset.\n",
    "    wv_intermed_idx = wv_intermed-350\n",
    "    X_train = X_train[:,wv_intermed_idx]\n",
    "\n",
    "    # Use another genetic algorithm to find the best wavebands out of the narrowed possibilities\n",
    "    print('\\tCONSENSUS:')\n",
    "    print('\\tStarting genetic algorithm...', end=' ')\n",
    "    wv_consensus = ga(X_train, y_train, trained_pipe, n_features=max_features_output, wv_subset=wv_intermed)\n",
    "    print('\\tDone.')\n",
    "    return (wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a49f65f-065b-47fc-b4f1-4eeb463189df",
   "metadata": {},
   "source": [
    "**The \"main\" function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5e1a93-cf0d-4df9-9166-deb913ef0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would normally be in the main function, but defined separately for easier testing, debugging, and analysis after running within a Jupyter notebook.\n",
    "def run():\n",
    "    # Lists of results that will be compiled at the end into a DataFrame for writing to CSV\n",
    "    region_list = []\n",
    "    gene_list = []\n",
    "    method_list = []\n",
    "    wv_list = []\n",
    "    rmse_list = []\n",
    "    r2_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    # Loop over both the whole waveband range and visible light only. (Making the loop in this format in case there are any other regions we want to add later.)\n",
    "    for starting_region in (\"all\", \"vis\"):\n",
    "\n",
    "        if(starting_region == \"all\":\n",
    "            X_train = X_all_train\n",
    "            X_test = X_all_test\n",
    "            X_phoa_train = X_all_phoa_train\n",
    "            X_phoa_test = X_all_phoa_test\n",
    "        elif(starting_region == \"vis\"):\n",
    "            X_train = X_vis_train\n",
    "            X_test = X_vis_test\n",
    "            X_phoa_train = X_vis_phoa_train\n",
    "            X_phoa_test = X_vis_phoa_test\n",
    "            \n",
    "        # Loop over each gene (y value)\n",
    "        for gene, y_train, y_test in zip((\"phoa\", \"cbblr\", \"fungi\", \"bact\", \"urec\"), (phoa_train, cbblr_train, fungi_train, bact_train, urec_train), (phoa_test, cbblr_test, fungi_test, bact_test, urec_test)):\n",
    "    \n",
    "            print(\"Starting \", gene, \"...\", sep = \"\")\n",
    "            \n",
    "            # Where the main calculations happen. Runs each method separately, and then finds the consensus of all of them.\n",
    "            # The phoa special case is due to a different train/test split than the rest because of some NANs.\n",
    "            if(gene == \"phoa\"):\n",
    "                wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus = consensus(X_phoa_train, y_train)\n",
    "            else:\n",
    "                wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus = consensus(X_train, y_train)\n",
    "            \n",
    "            for method, wv_set in zip((\"mi\", \"coeffs\", \"ga\", \"cluster\", \"perm_imp\", \"consensus\"), (wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus)):\n",
    "                \n",
    "                # Build a new elastic net model for validation on this subset of wavebands\n",
    "                wv_set_idx = wv_set-350\n",
    "                validator = GridSearchCV(estimator=pipe_elastic_net, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "                # Like above, special case for phoa\n",
    "                if(gene == \"phoa\"):\n",
    "                    validator.fit(X_phoa_train[:,wv_set_idx], y_train)\n",
    "                    preds = validator.predict(X_phoa_test[:,wv_set_idx])\n",
    "                    #rmse = validator.score(X_phoa_test[:,wv_set_idx], y_test) * -1\n",
    "                    rmse = root_mean_squared_error(y_test, preds) * -1\n",
    "                    r2 = r2_score(y_test, preds)\n",
    "                    mae = mean_absolute_error(y_test, preds)\n",
    "                else:\n",
    "                    validator.fit(X_train[:,wv_set_idx], y_train)\n",
    "                    preds = validator.predict(X_test[:,wv_set_idx])\n",
    "                    rmse = root_mean_squared_error(y_test, preds) * -1\n",
    "                    r2 = r2_score(y_test, preds)\n",
    "                    mae = mean_absolute_error(y_test, preds)\n",
    "    \n",
    "                # Record each waveband separately. This is difficult to analyze visually, but makes for MUCH easier analysis in R later.\n",
    "                for wv in wv_set:\n",
    "                    region_list.append(starting_region)\n",
    "                    gene_list.append(gene)\n",
    "                    method_list.append(method)\n",
    "                    wv_list.append(wv)\n",
    "                    rmse_list.append(rmse)\n",
    "                    \n",
    "            print(\"Finished \", gene, \".\", sep = \"\")\n",
    "\n",
    "    # Compile the results into a single DataFrame and write it to a CSV\n",
    "    # The format will make for easier analysis later.\n",
    "    print(\"Compiling and writing results to CSV...\", end = \" \")\n",
    "    col_names = ['gene', 'method', 'wv', 'rmse']\n",
    "    results = pd.DataFrame(columns = col_names)\n",
    "    results['region'] = region_list\n",
    "    results['gene'] = gene_list\n",
    "    results['method'] = method_list\n",
    "    results['wv'] = wv_list\n",
    "    results['rmse'] = rmse_list\n",
    "    results['r2'] = r2_list\n",
    "    results['mae'] = mae_list\n",
    "    results.to_csv(root + '//results/results.csv', index=False)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ab9f9-5fe6-4e67-a177-e48025b4106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
