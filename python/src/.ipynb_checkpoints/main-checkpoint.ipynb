{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd91d8c-be40-4c4f-b55a-2aa13f2708f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn_genetic import GAFeatureSelectionCV\n",
    "# from sklearn_genetic.space import Categorical, Integer, Continuous\n",
    "\n",
    "# from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c35222-7cae-4d10-8aa1-93889b0117f6",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d6faae-0195-426f-90d5-db5b48f055dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether using data_consol.csv helps anything. If so, probably indicates an error in reading in or joining the separate CSVs before\n",
    "repo = git.Repo('.', search_parent_directories = True)\n",
    "root = repo.working_tree_dir\n",
    "\n",
    "data_consol = pd.read_csv(root + '//data/data_consol.csv')\n",
    "\n",
    "SEED = 0\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# Intended for reproducible GA steps\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "X = data_consol.filter(regex=\"^[0-9]+$\")\n",
    "    \n",
    "# Note: do NOT scale X and y before splitting, since that is a data leak. Instead, use the pipeline to scale both Xs, and separately scale the y for custom scoring like RMSE.\n",
    "X_train, X_test, y_train_unscaled, y_test_unscaled = train_test_split(data_consol.filter(regex=\"^[0-9]+$\").to_numpy(), data_consol.filter(regex=\"pcr_[a-z]+_log\"), train_size=0.8, random_state=0)\n",
    "\n",
    "# Separate y by genes. Reshaping necessary for the y scaling step\n",
    "bact_train_unscaled = y_train_unscaled[\"pcr_bact_log\"].to_numpy().reshape(-1,1)\n",
    "bact_test_unscaled = y_test_unscaled[\"pcr_bact_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "cbblr_train_unscaled = y_train_unscaled[\"pcr_cbblr_log\"].to_numpy().reshape(-1,1)\n",
    "cbblr_test_unscaled = y_test_unscaled[\"pcr_cbblr_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "fungi_train_unscaled = y_train_unscaled[\"pcr_fungi_log\"].to_numpy().reshape(-1,1)\n",
    "fungi_test_unscaled = y_test_unscaled[\"pcr_fungi_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "phoa_train_unscaled = y_train_unscaled[\"pcr_phoa_log\"].to_numpy().reshape(-1,1)\n",
    "phoa_test_unscaled = y_test_unscaled[\"pcr_phoa_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "urec_train_unscaled = y_train_unscaled[\"pcr_urec_log\"].to_numpy().reshape(-1,1)\n",
    "urec_test_unscaled = y_test_unscaled[\"pcr_urec_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "# Scale each y with respect to its distribution\n",
    "\n",
    "bact_scaler = StandardScaler()\n",
    "bact_train = bact_scaler.fit_transform(bact_train_unscaled).reshape(-1,1)\n",
    "bact_test = bact_scaler.transform(bact_test_unscaled).reshape(-1,1)\n",
    "\n",
    "cbblr_scaler = StandardScaler()\n",
    "cbblr_train = cbblr_scaler.fit_transform(cbblr_train_unscaled).reshape(-1,1)\n",
    "cbblr_test = cbblr_scaler.transform(cbblr_test_unscaled).reshape(-1,1)\n",
    "\n",
    "fungi_scaler = StandardScaler()\n",
    "fungi_train = fungi_scaler.fit_transform(fungi_train_unscaled).reshape(-1,1)\n",
    "fungi_test = fungi_scaler.transform(fungi_test_unscaled).reshape(-1,1)\n",
    "\n",
    "phoa_scaler = StandardScaler()\n",
    "phoa_train = phoa_scaler.fit_transform(phoa_train_unscaled).reshape(-1,1)\n",
    "phoa_test = phoa_scaler.transform(phoa_test_unscaled).reshape(-1,1)\n",
    "\n",
    "urec_scaler = StandardScaler()\n",
    "urec_train = urec_scaler.fit_transform(urec_train_unscaled).reshape(-1,1)\n",
    "urec_test = urec_scaler.transform(urec_test_unscaled).reshape(-1,1)\n",
    "\n",
    "# 5-fold CV; random state 0\n",
    "cv_5_0 = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Used for waveband selection\n",
    "wvs = np.arange(350,2501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92808522-e1a1-4167-842a-913c8fadcd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is only with respect to X_train, not any of the target variables, this only has to be computed once. (It's relatively cheap to compute, but this also has the benefit of preserving the random choices.)\n",
    "def cluster(X_train):\n",
    "    \"\"\" Uses agglomerative clustering with a distance threshold of 0.999 on the normalized feature correlation coefficient matrix. Then, it randomly selects one waveband from each cluster.\n",
    "    This should be used as a preprocessing step when doing permutation importance. (Clustering method) \"\"\"\n",
    "    corr = np.corrcoef(X.T) # X needs to be transposed because of corrcoef's implementation\n",
    "    agg = AgglomerativeClustering(n_clusters=None, distance_threshold=0.999) # The distance threshold is somewhat arbitrary, but it's based on EDA and domain knowledge, and the results seem reasonable.\n",
    "    clusters = agg.fit_predict(corr)\n",
    "    # Now select a single \"representative\" waveband from each cluster\n",
    "    cluster_choices = []\n",
    "    for i in range(np.max(clusters)):\n",
    "        wv_in_cluster = wvs[clusters==i]\n",
    "        cluster_choices.append(rng.choice(wv_in_cluster))\n",
    "    cluster_choices = np.sort(np.array(cluster_choices))\n",
    "    return cluster_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43ac874-fce3-4b88-b42e-00897bae084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_choices = cluster(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80f2e2",
   "metadata": {},
   "source": [
    "**The major pipeline components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a96e4672-3926-402d-b16c-85ec2caa98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet(fit_intercept=False, warm_start=True, random_state=0, selection='random', max_iter=4000)\n",
    "\n",
    "# Used for embedded feature importance (via coeffs) and wrapper feature importance (via perm importance)\n",
    "pipe_elastic_net = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"elastic_net\", elastic_net)\n",
    "    ],\n",
    "    memory = root+'\\\\cache',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Hyperparameters for elastic net tuning. When code is finalized, expand for more thorough search using more computational resources.\n",
    "REGULARIZATION = np.logspace(-5, 0, 8)\n",
    "MIXTURE = np.linspace(0.001, 1, 8)\n",
    "PARAM_GRID = [\n",
    "    {\n",
    "        \"elastic_net__alpha\": REGULARIZATION,\n",
    "        \"elastic_net__l1_ratio\": MIXTURE\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d64d3-904a-4f44-a923-cb8ce27dd59a",
   "metadata": {},
   "source": [
    "**Feature selection functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef09dedf-8f50-4c85-951d-c911328949bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi(X_train, y_train, n_features=64):\n",
    "    \"\"\" Uses mutual information to calculate the n_features most related features in X_train to y_train. (Filter method) \"\"\"\n",
    "    y_train = y_train.ravel()\n",
    "    mi = mutual_info_regression(X_train, y_train)\n",
    "    top_n_idx = np.argpartition(mi, -n_features)[-n_features:]\n",
    "    return wvs[top_n_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c05613-121e-4666-ba40-c285cf7c4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_elastic_net(X_train, y_train):\n",
    "    \"\"\" Builds and fits an elastic net model using all features. \n",
    "    Returns the fit estimator (a pipeline). Used within coeffs() and ga(). \"\"\"\n",
    "    grid = GridSearchCV(estimator=pipe_elastic_net, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "    grid.fit(X_train, y_train)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e5f4d2-3b6f-433f-92df-b2c8279ba4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeffs(estimator, n_features=64):\n",
    "    \"\"\" Builds and fits an elastic net model using all features. Returns the n_features features with the highest absolute-valued coefficients. (Embedded method) \"\"\"\n",
    "    coeffs = estimator['elastic_net'].coef_\n",
    "    abs_coeffs = np.abs(coeffs)\n",
    "    top_n_idx = np.argpartition(abs_coeffs, -n_features)[-n_features:]\n",
    "    return wvs[top_n_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "693601be-723e-4ce0-b1a5-676fbdd59413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ga(X_train, y_train, trained_estimator, n_features=64, wv_subset=None):\n",
    "    \"\"\" Uses a genetic algorithm to find the wavebands that gives the lowest RMSE on an elastic net model. \n",
    "    The subset will be at most n_features large, but it may be less than n_features large. \n",
    "    wv_subset should be None when in the feature selection layer, but when in the consolidation layer, it should\n",
    "    be the subset of possible wavelengths output by the concatenated feature selection methods, not the entire\n",
    "    [350,2500] set. (GA method) \"\"\"\n",
    "    \n",
    "    y_train = y_train.ravel()\n",
    "    ga_selector = GAFeatureSelectionCV(\n",
    "        estimator=trained_estimator,\n",
    "        cv=cv_5_0,  # Cross-validation folds\n",
    "        scoring=\"neg_root_mean_squared_error\",  # Fitness function (maximize accuracy)\n",
    "        population_size=n_features*2,  # Number of individuals in the population\n",
    "        generations=50,  # Number of generations\n",
    "        n_jobs=-1,  # Use all available CPU cores\n",
    "        verbose=False,\n",
    "        max_features=n_features,\n",
    "        return_train_score=True,\n",
    "        refit=False,\n",
    "        crossover_probability=0.8,\n",
    "        mutation_probability=0.2\n",
    "    )\n",
    "    pipe_ga = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"ga\", ga_selector)\n",
    "        ], \n",
    "        memory = root+'\\\\cache',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    pipe_ga.fit(X_train, y_train)\n",
    "    feats = pipe_ga['ga'].best_features_ # A mask of the features selected from X_train\n",
    "\n",
    "    # Should be the case in the feature selection layer\n",
    "    if wv_subset is None:\n",
    "        return wvs[feats]\n",
    "    # Should be the case in the consensus layer\n",
    "    else:\n",
    "        return wv_subset[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78bd175-1cf6-4533-a305-dacce062f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_imp(X_train, y_train, n_features=64):\n",
    "    \"\"\" Calculates permutation importance on a dataset. cluster_choices should be the result of calling cluster(), which should be done once at the start of execution. \n",
    "    This is done outside this function to preserve the random selection. Returns the set of n_features wavebands with the highest permutation importance on the training set. (Wrapper method) \"\"\"\n",
    "    # Use only the features selected by clustering\n",
    "    cluster_idx = cluster_choices - 350\n",
    "    X_train = X_train[:,cluster_idx]\n",
    "    # Build and train another elastic net model, but only on the features left after clustering, to use for permutation importance.\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"elastic_net\", elastic_net)\n",
    "        ], \n",
    "        memory = root+'\\\\cache',\n",
    "        verbose=False\n",
    "    )    \n",
    "    grid = GridSearchCV(estimator=pipe, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "    grid.fit(X_train, y_train)\n",
    "    perm_imp = permutation_importance(grid, X_train, y_train, scoring='neg_root_mean_squared_error', n_repeats=10, n_jobs=-1, random_state=0)\n",
    "    pi_top_64_idx = np.argpartition(perm_imp.importances_mean, -64)[-64:]\n",
    "    return cluster_choices[pi_top_64_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465453b6-7b18-40c6-b88a-502d1a9f4c9e",
   "metadata": {},
   "source": [
    "**Consensus function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29ed9401-293e-4afd-82e3-45e57f1bccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus(X_train, y_train, n_features_intermed=64, max_features_output=8):\n",
    "    \"\"\" Takes the wavebands output by the feature selection functions and uses a (separate) genetic algorithm to find the wavebands that give the lowest RMSE on an elastic net model.\n",
    "    The subset will be at most n_features large, but it may be less than n_features large.\n",
    "    Returns the tuple: (wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus), where each is a numpy array of wavebands that were selected by each method.  (Consensus method) \"\"\"\n",
    "    \n",
    "    print('\\tFEATURE SELECTION:')\n",
    "    print('\\tStarting mutual importance...', end=' ')\n",
    "    wv_mi = mi(X_train, y_train, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "    print('\\tTraining the elastic net model...', end=' ')\n",
    "    trained_pipe = train_elastic_net(X_train, y_train)\n",
    "    wv_coeffs = coeffs(trained_pipe, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "    print('\\tStarting genetic algorithm...', end=' ')\n",
    "    wv_ga = ga(X_train, y_train, trained_pipe, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "    print('\\tStarting permutation importance...', end=' ')\n",
    "    wv_cluster = cluster_choices # Doesn't require a separate function call\n",
    "    wv_perm_imp = perm_imp(X_train, y_train, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "\n",
    "    # Compile the above results into one array, remove any duplicates, and sort.\n",
    "    wv_intermed = np.append(wv_mi, wv_coeffs)\n",
    "    wv_intermed = np.append(wv_intermed, wv_ga)\n",
    "    wv_intermed = np.append(wv_intermed, wv_cluster)\n",
    "    wv_intermed = np.append(wv_intermed, wv_perm_imp)\n",
    "    wv_intermed = np.sort(np.unique(wv_intermed))\n",
    "\n",
    "    # Convert the above into indices for masking over the dataset.\n",
    "    wv_intermed_idx = wv_intermed-350\n",
    "    X_train = X_train[:,wv_intermed_idx]\n",
    "\n",
    "    # Use another genetic algorithm to find the best wavebands out of the narrowed possibilities\n",
    "    print('\\tCONSENSUS:')\n",
    "    print('\\tStarting genetic algorithm...', end=' ')\n",
    "    wv_consensus = ga(X_train, y_train, trained_pipe, n_features=max_features_output, wv_subset=wv_intermed)\n",
    "    print('\\tDone.')\n",
    "    return (wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a49f65f-065b-47fc-b4f1-4eeb463189df",
   "metadata": {},
   "source": [
    "**The \"main\" function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d5e1a93-cf0d-4df9-9166-deb913ef0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would normally be in the main function, but defined separately for easier testing, debugging, and analysis after running within a Jupyter notebook.\n",
    "def run():\n",
    "    # Lists of results that will be compiled at the end into a DataFrame for writing to CSV\n",
    "    gene_list = []\n",
    "    method_list = []\n",
    "    wv_list = []\n",
    "    rmse_list = []\n",
    "\n",
    "    # Loop over each gene (y value)\n",
    "    for gene, y_train, y_test in zip((\"bact\", \"cbblr\", \"fungi\", \"phoa\", \"urec\"), (bact_train, cbblr_train, fungi_train, phoa_train, urec_train), (bact_test, cbblr_test, fungi_test, phoa_test, urec_test)):\n",
    "\n",
    "        print(\"Starting \", gene, \"...\", sep = \"\")\n",
    "        \n",
    "        # Where the main calculations happen. Runs each method separately, and then finds the consensus of all of them.\n",
    "        wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus = consensus(X_train, y_train)\n",
    "        \n",
    "        for method, wv_set in zip((\"mi\", \"coeffs\", \"ga\", \"cluster\", \"perm_imp\"), (wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus)):\n",
    "            \n",
    "            # Build a new elastic net model for validation on this subset of wavebands\n",
    "            wv_set_idx = wv_set-350\n",
    "            validator = GridSearchCV(estimator=pipe_elastic_net, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "            validator.fit(X_train[:,wv_set_idx], y_train)\n",
    "            rmse = validator.score(X_test[:,wv_set_idx], y_test)\n",
    "\n",
    "            # Record each waveband separately. This is difficult to analyze visually, but makes for MUCH easier analysis in R later.\n",
    "            for wv in wv_set:\n",
    "                gene_list.append(gene)\n",
    "                method_list.append(method)\n",
    "                wv_list.append(wv)\n",
    "                rmse_list.append(rmse)\n",
    "                \n",
    "        print(\"Finished \", gene, \".\", sep = \"\")\n",
    "\n",
    "    # Compile the results into a single DataFrame and write it to a CSV\n",
    "    # The format will make for easier analysis later.\n",
    "    print(\"Compiling and writing results to CSV...\", end = \" \")\n",
    "    col_names = ['gene', 'method', 'wv', 'rmse']\n",
    "    results = pd.DataFrame(columns = col_names)\n",
    "    results['gene'] = gene_list\n",
    "    results['method'] = method_list\n",
    "    results['wv'] = wv_list\n",
    "    results['rmse'] = rmse_list\n",
    "    results.to_csv(root + '//results/results.csv', index=False)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d70ab9f9-5fe6-4e67-a177-e48025b4106b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n",
      "Starting bact...\n",
      "\tFEATURE SELECTION:\n",
      "\tStarting mutual importance... Done.\n",
      "\tTraining the elastic net model... Done.\n",
      "\tStarting genetic algorithm... Done.\n",
      "\tStarting permutation importance... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.444e+01, tolerance: 3.180e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "wv_mi: [1876 1653  406 1651 2295 1442 2004 1444 1437 1443 1620 2330 1581 2312\n",
      " 2256 1436 2254 1582  716 2007 1643 1646 1621 1642  727 2262 1645 2314\n",
      " 1644 1622 1639 2258 1886 2257 1641 1638 2008 2313 1637 1640  717 1623\n",
      " 2261 1636 1624 1635 1632 2259 1634 2260 1633 1885  983 1625  720 1626\n",
      " 1630 1629 1631  719  718 1627 1628  354]\n",
      "wv_coeffs: [1948 2347  430  382 2350  394  396  364  472 1925  981 1947 1867 1871\n",
      "  428  366  479 2349 1884 1902 2348 1939  971 1868 1870  476 1934 1916\n",
      " 1938  429 1869 1946  473  365 1899 1917  980  477  395  973 1890  478\n",
      " 1945 1940 1935 1937  979 1936 1901 1944  978 1900  975  977 1941 1943\n",
      "  976 1885  974 1942 1889 1886 1888 1887]\n",
      "wv_ga: [ 394  402  508  564  569  592  596  627  682  683  688  743  747  900\n",
      "  949  974  976  978 1110 1167 1239 1258 1331 1366 1369 1417 1419 1429\n",
      " 1454 1504 1636 1695 1740 1800 1802 1807 1838 1854 1865 1866 1867 1871\n",
      " 1874 1881 1882 1884 1890 1902 1908 1915 1922 1937 1940 1941 1942 1944\n",
      " 1947 1964 1971 2090 2111 2231 2474]\n",
      "wv_cluster: [ 350  351  352  353  354  355  356  357  358  359  360  362  363  367\n",
      "  370  373  383  385  391  397  401  406  409  421  430  450  457  474\n",
      "  489  496  498  511  512  519  525  531  540  549  562  566  581  603\n",
      "  615  632  642  647  653  659  660  663  666  667  678  690  691  693\n",
      "  694  697  700  707  708  710  712  715  717  720  721  724  726  730\n",
      "  736  742  747  780  800  925  959  962  984  985 1093 1150 1218 1282\n",
      " 1321 1333 1344 1361 1365 1380 1382 1386 1391 1395 1398 1405 1410 1430\n",
      " 1450 1486 1499 1507 1514 1523 1531 1539 1553 1567 1577 1600 1665 1696\n",
      " 1705 1715 1746 1750 1769 1786 1827 1845 1849 1857 1862 1868 1873 1880\n",
      " 1883 1886 1887 1888 1890 1892 1895 1896 1898 1903 1904 1911 1913 1920\n",
      " 1930 1946 1954 1957 1969 1975 1983 1987 2004 2018 2025 2044 2051 2082\n",
      " 2091 2109 2117 2120 2133 2139 2150 2155 2171 2196 2240 2247 2253 2270\n",
      " 2280 2294 2308 2333 2334 2361 2379 2384 2409 2429 2432 2456 2465 2471\n",
      " 2482 2491]\n",
      "wv_perm_imp: [2384 2429 2044  391  962 1769 2334 2456 2379 2294  717 2270  985  401\n",
      " 2409  715  724 1391 1715 2240 1903 1365 1398 1218 1857  385  736  747\n",
      " 2091  730 1873 1969  457 2150  474 1395 1975  383 2253 1898  450  397\n",
      "  496  742 2139  489 1987  540 1849  726  925 2018  678 2051 1696 1886\n",
      " 1410 1827  430 1430 1845 1665 1868 1946]\n",
      "\tCONSENSUS:\n",
      "\tStarting genetic algorithm... \tDone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.070e+02, tolerance: 3.180e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.313e+01, tolerance: 3.180e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.085e+01, tolerance: 3.180e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.444e+01, tolerance: 3.180e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.391e+01, tolerance: 3.180e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished bact.\n",
      "Starting cbblr...\n",
      "\tFEATURE SELECTION:\n",
      "\tStarting mutual importance... Done.\n",
      "\tTraining the elastic net model... Done.\n",
      "\tStarting genetic algorithm... Done.\n",
      "\tStarting permutation importance... Done.\n",
      "wv_mi: [2261 1887  731 2269 2001 2272 1890 2006 1997 2000 2046 2051 2047 2048\n",
      " 2007 1892 2044 2049 2045 2005 2270 2050 2002 2043 2271 2004  698 2034\n",
      " 2035 2003 2008 2025 2026 2027 2033 2009  697 2042 2040 2041 2014 2011\n",
      " 2024 2028 2015 2013 2010 2039 2012 2036 2038 2032 2031 2029 2037 2016\n",
      " 2022 2023 2030 2017 2018 2021 2019 2020]\n",
      "wv_coeffs: [2469  363 2473 2467 2468 2348 2331 1886  356  522  355 1908  502 2332\n",
      " 1919 1906 1907 2334 1904 1905  501  503  361 2333  521 2480  366 2474\n",
      " 1920  520  358  505  504  506  519 1887  518  507 1890  353  517 2479\n",
      "  508  364  509  516  510  513  365  512  511  515 1888 2475  514  360\n",
      " 1889  359 2478 2476 2477  351  352  350]\n",
      "wv_ga: [ 350  351  360  436  448  512  513  517  519  520  549  551  556  560\n",
      "  570  799  885  914  923  931  938  962 1013 1088 1127 1165 1266 1269\n",
      " 1310 1371 1512 1513 1526 1536 1539 1570 1592 1602 1604 1725 1759 1907\n",
      " 1909 1947 1976 1984 1989 1992 2005 2036 2048 2050 2078 2252 2261 2292\n",
      " 2308 2322 2336 2338 2339 2343 2350 2353]\n",
      "wv_cluster: [ 350  351  352  353  354  355  356  357  358  359  360  362  363  367\n",
      "  370  373  383  385  391  397  401  406  409  421  430  450  457  474\n",
      "  489  496  498  511  512  519  525  531  540  549  562  566  581  603\n",
      "  615  632  642  647  653  659  660  663  666  667  678  690  691  693\n",
      "  694  697  700  707  708  710  712  715  717  720  721  724  726  730\n",
      "  736  742  747  780  800  925  959  962  984  985 1093 1150 1218 1282\n",
      " 1321 1333 1344 1361 1365 1380 1382 1386 1391 1395 1398 1405 1410 1430\n",
      " 1450 1486 1499 1507 1514 1523 1531 1539 1553 1567 1577 1600 1665 1696\n",
      " 1705 1715 1746 1750 1769 1786 1827 1845 1849 1857 1862 1868 1873 1880\n",
      " 1883 1886 1887 1888 1890 1892 1895 1896 1898 1903 1904 1911 1913 1920\n",
      " 1930 1946 1954 1957 1969 1975 1983 1987 2004 2018 2025 2044 2051 2082\n",
      " 2091 2109 2117 2120 2133 2139 2150 2155 2171 2196 2240 2247 2253 2270\n",
      " 2280 2294 2308 2333 2334 2361 2379 2384 2409 2429 2432 2456 2465 2471\n",
      " 2482 2491]\n",
      "wv_perm_imp: [1567  496  421 1410 1553 1600 2247  525 1531 2379  653 1365 1890  647\n",
      "  498 2025 2004 1983  736  355  367  925 1361 1911  383 2051  359  962\n",
      "  350 1987  360 2044  959  457  742  747 1715  800  780 2482  353  450\n",
      " 1903  566 2361  373  430  540  352 1886  562 2308 2471 1888 1887 1904\n",
      " 2294  549 1920  519 2334  512 2333  511]\n",
      "\tCONSENSUS:\n",
      "\tStarting genetic algorithm... \tDone.\n",
      "Finished cbblr.\n",
      "Starting fungi...\n",
      "\tFEATURE SELECTION:\n",
      "\tStarting mutual importance... Done.\n",
      "\tTraining the elastic net model... Done.\n",
      "\tStarting genetic algorithm... Done.\n",
      "\tStarting permutation importance... Done.\n",
      "wv_mi: [2425 1480 1451 1479 1450 2175 1482 2179 1552 1458 1459 2167 2178 1481\n",
      " 2187 1476 1542 1447 2421 2166 1550 1551 1449 1477 1460 1483 1448 2181\n",
      " 1474 2439 2177 1484 1472 1461 2180 1473 2438 1475 2182 1462 1543 1464\n",
      " 1463 1548 1471 1549 1546 2424 1547 1465 1544 1886 2185 1469 2186 2183\n",
      " 1468 1470 2422 1467 1466 1545 2423 2184]\n",
      "wv_coeffs: [2330 2026  622 2024 2305 2025 1365  631  553  623  512 2333 2303  539\n",
      "  433 1359  510 2014 2010 2331  626 2304  552 2332 1364  551  630  629\n",
      "  625  547 1360  380  546 1363  545  624  541  508  548  544  540  628\n",
      "  627 1362  550 1361  509  549  437  511  432  434  543  542 2011  436\n",
      " 2013  435  352 2012  350  361  351  360]\n",
      "wv_ga: [ 352  359  433  435  437  464  509  520  539  545  546  547  551  553\n",
      "  555  622  623  628  632  633  660  763  775  784  812  882  946  982\n",
      " 1096 1102 1125 1182 1213 1355 1377 1378 1425 1440 1538 1672 1695 1767\n",
      " 2039 2043 2044 2052 2070 2083 2091 2116 2149 2170 2219 2303 2306 2343]\n",
      "wv_cluster: [ 350  351  352  353  354  355  356  357  358  359  360  362  363  367\n",
      "  370  373  383  385  391  397  401  406  409  421  430  450  457  474\n",
      "  489  496  498  511  512  519  525  531  540  549  562  566  581  603\n",
      "  615  632  642  647  653  659  660  663  666  667  678  690  691  693\n",
      "  694  697  700  707  708  710  712  715  717  720  721  724  726  730\n",
      "  736  742  747  780  800  925  959  962  984  985 1093 1150 1218 1282\n",
      " 1321 1333 1344 1361 1365 1380 1382 1386 1391 1395 1398 1405 1410 1430\n",
      " 1450 1486 1499 1507 1514 1523 1531 1539 1553 1567 1577 1600 1665 1696\n",
      " 1705 1715 1746 1750 1769 1786 1827 1845 1849 1857 1862 1868 1873 1880\n",
      " 1883 1886 1887 1888 1890 1892 1895 1896 1898 1903 1904 1911 1913 1920\n",
      " 1930 1946 1954 1957 1969 1975 1983 1987 2004 2018 2025 2044 2051 2082\n",
      " 2091 2109 2117 2120 2133 2139 2150 2155 2171 2196 2240 2247 2253 2270\n",
      " 2280 2294 2308 2333 2334 2361 2379 2384 2409 2429 2432 2456 2465 2471\n",
      " 2482 2491]\n",
      "wv_perm_imp: [1904 1903 1896 1898 1895 1892 1888 1887 2465 2471 2482 2491 2456 2432\n",
      " 2429 2409 2280 2384 2379 2361 2196 2171 2155 2150 2270 2253 2247 2133\n",
      " 2117 2109 2091 2082 2240  691 1983  421  615 1321 2294 2051 1890  383\n",
      "  362 2018  512 1987  359  352 2308  511 1365 2334 2044 2004 1361 2333\n",
      "  350  632 2025  430  540  549  351  360]\n",
      "\tCONSENSUS:\n",
      "\tStarting genetic algorithm... \tDone.\n",
      "Finished fungi.\n",
      "Starting phoa...\n",
      "\tFEATURE SELECTION:\n",
      "\tStarting mutual importance... "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m run()\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting \u001b[39m\u001b[38;5;124m\"\u001b[39m, gene, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Where the main calculations happen. Runs each method separately, and then finds the consensus of all of them.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus \u001b[38;5;241m=\u001b[39m consensus(X_train, y_train)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method, wv_set \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoeffs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mga\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperm_imp\u001b[39m\u001b[38;5;124m\"\u001b[39m), (wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus)):\n\u001b[0;32m     20\u001b[0m     \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Build a new elastic net model for validation on this subset of wavebands\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     wv_set_idx \u001b[38;5;241m=\u001b[39m wv_set\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m350\u001b[39m\n",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m, in \u001b[0;36mconsensus\u001b[1;34m(X_train, y_train, n_features_intermed, max_features_output)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mFEATURE SELECTION:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mStarting mutual importance...\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m wv_mi \u001b[38;5;241m=\u001b[39m mi(X_train, y_train, n_features\u001b[38;5;241m=\u001b[39mn_features_intermed)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTraining the elastic net model...\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m, in \u001b[0;36mmi\u001b[1;34m(X_train, y_train, n_features)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Uses mutual information to calculate the n_features most related features in X_train to y_train. (Filter method) \"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m----> 4\u001b[0m mi \u001b[38;5;241m=\u001b[39m mutual_info_regression(X_train, y_train)\n\u001b[0;32m      5\u001b[0m top_n_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margpartition(mi, \u001b[38;5;241m-\u001b[39mn_features)[\u001b[38;5;241m-\u001b[39mn_features:]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wvs[top_n_idx]\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:441\u001b[0m, in \u001b[0;36mmutual_info_regression\u001b[1;34m(X, y, discrete_features, n_neighbors, copy, random_state, n_jobs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    326\u001b[0m     {\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    346\u001b[0m ):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Estimate mutual information for a continuous target variable.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m    Mutual information (MI) [1]_ between two random variables is a non-negative\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m    array([0.1..., 2.6...  , 0.0...])\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _estimate_mi(\n\u001b[0;32m    442\u001b[0m         X,\n\u001b[0;32m    443\u001b[0m         y,\n\u001b[0;32m    444\u001b[0m         discrete_features\u001b[38;5;241m=\u001b[39mdiscrete_features,\n\u001b[0;32m    445\u001b[0m         discrete_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    446\u001b[0m         n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors,\n\u001b[0;32m    447\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    448\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[0;32m    449\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    450\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:271\u001b[0m, in \u001b[0;36m_estimate_mi\u001b[1;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state, n_jobs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_estimate_mi\u001b[39m(\n\u001b[0;32m    203\u001b[0m     X,\n\u001b[0;32m    204\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    212\u001b[0m ):\n\u001b[0;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Estimate mutual information between the features and the target.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m           Data Sets\". PLoS ONE 9(2), 2014.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m discrete_target)\n\u001b[0;32m    272\u001b[0m     n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(discrete_features, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m)):\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\utils\\validation.py:1318\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[0;32m   1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1302\u001b[0m     X,\n\u001b[0;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1316\u001b[0m )\n\u001b[1;32m-> 1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\utils\\validation.py:1340\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1338\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1339\u001b[0m     y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1340\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[0;32m   1341\u001b[0m     _ensure_no_complex_data(y)\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    124\u001b[0m     X,\n\u001b[0;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    130\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
