{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd91d8c-be40-4c4f-b55a-2aa13f2708f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn_genetic import GAFeatureSelectionCV\n",
    "from sklearn_genetic.space import Categorical, Integer, Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c35222-7cae-4d10-8aa1-93889b0117f6",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600effed-2189-4cfa-84f8-1a317803aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether using data_consol.csv helps anything. If so, probably indicates an error in reading in or joining the separate CSVs before\n",
    "repo = git.Repo('.', search_parent_directories = True)\n",
    "root = repo.working_tree_dir\n",
    "\n",
    "data_consol = pd.read_csv(root + '//data/data_consol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d52a769-326d-4c99-ae7f-eed98a407bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_consol.filter(regex=\"^[0-9]+$\")\n",
    "bact = data_consol['pcr_bact_log']\n",
    "\n",
    "# Note: do NOT scale X and y before splitting, since that is a data leak. Instead, use the pipeline to scale both Xs, and separately scale the y for custom scoring like RMSE.\n",
    "X_train, X_test, bact_train_unscaled, bact_test_unscaled = train_test_split(X.to_numpy(), bact.to_numpy(), train_size=0.8, random_state=0)\n",
    "\n",
    "# Reshaping necessary for the y scaling step\n",
    "bact_train_unscaled = bact_train_unscaled.reshape(-1,1)\n",
    "bact_test_unscaled = bact_test_unscaled.reshape(-1,1)\n",
    "\n",
    "bact_scaler = StandardScaler()\n",
    "bact_train = bact_scaler.fit_transform(bact_train_unscaled).reshape(-1,1)\n",
    "bact_test = bact_scaler.transform(bact_test_unscaled).reshape(-1,1)\n",
    "\n",
    "# 10-fold CV; random state 0\n",
    "cv_5_0 = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80f2e2",
   "metadata": {},
   "source": [
    "**The major pipeline components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96e4672-3926-402d-b16c-85ec2caa98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the genetic algorithm feature selector\n",
    "# elastic_net = ElasticNet(fit_intercept=False, warm_start=True, random_state=0, selection='random', max_iter=4000)\n",
    "\n",
    "# ga_selector = GAFeatureSelectionCV(\n",
    "#     estimator=elastic_net,\n",
    "#     cv=cv_5_0,  # Cross-validation folds\n",
    "#     scoring=\"neg_root_mean_squared_error\",  # Fitness function (maximize accuracy)\n",
    "#     population_size=20,  # Number of individuals in the population\n",
    "#     generations=50,  # Number of generations\n",
    "#     n_jobs=-1,  # Use all available CPU cores\n",
    "#     verbose=True,  # Print progress\n",
    "#     max_features = 32\n",
    "# )\n",
    "\n",
    "# pipe = Pipeline(\n",
    "#     [\n",
    "#         (\"scaler\", StandardScaler()),\n",
    "#         #(\"features\", ga_selector) ,\n",
    "#         (\"elastic_net\", elastic_net)\n",
    "#     ], \n",
    "#     memory = root+'\\\\cache',\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# REGULARIZATION = np.logspace(-5, 0, 8)\n",
    "# MIXTURE = np.linspace(0.001, 1, 8)\n",
    "# PARAM_GRID = [\n",
    "#     {\n",
    "#         \"elastic_net__alpha\": REGULARIZATION,\n",
    "#         \"elastic_net__l1_ratio\": MIXTURE\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# grid = GridSearchCV(estimator=pipe, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d64d3-904a-4f44-a923-cb8ce27dd59a",
   "metadata": {},
   "source": [
    "**Train the model(s)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410ac9cb-424b-442d-af59-6f76fcb0c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid.fit(X_train, bact_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e9c61-e19d-4fa6-a301-9aeff04bbfd1",
   "metadata": {},
   "source": [
    "**Investigate results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c5b2a5-05bf-4486-9270-0bc2b0534a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Training RMSE:', round(abs(grid.score(X_train, bact_train)), 3))\n",
    "# print('Testing RMSE:', round(abs(grid.score(X_test, bact_test)), 3))\n",
    "\n",
    "# # Inverse-transforming the preds to get back to original scale.\n",
    "# # Used for comparison with R results\n",
    "# preds_unscaled = bact_scaler.inverse_transform(grid.predict(X_test).reshape(-1,1))\n",
    "# print('Testing RMSE, unscaled:', round(root_mean_squared_error(preds_unscaled, bact_test_unscaled), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c732adc1-3751-40ff-83a4-2c801bb47e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs = np.arange(350,2501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef09dedf-8f50-4c85-951d-c911328949bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2256 2003 2002 1438 1435 1482 2005 2312 2006  727 1441 1621 1483 1445\n",
      " 2254 1646 2314 1643 1642 1443 1444 1442 2257 1622 2262 1645 1436 1437\n",
      " 2004 2007 1638 1639 1644 2258 1641 2313 1623 1637 1640 2008  717 2261\n",
      " 1624 1886 1636  983 1635 2259 1632 1634 2260 1625 1633 1626  720 1631\n",
      " 1629 1630 1885 1628 1627  719  718  354]\n"
     ]
    }
   ],
   "source": [
    "# Filter: using mutual information criterion\n",
    "mi = mutual_info_regression(X_train, bact_train.ravel())\n",
    "mi_top_64_idx = np.argpartition(mi, -64)[-64:]\n",
    "print(wvs[mi_top_64_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e5f4d2-3b6f-433f-92df-b2c8279ba4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeffs (embedded method) choices\n",
    "\n",
    "# coeffs = grid.best_estimator_['elastic_net'].coef_\n",
    "# print(coeffs)\n",
    "# print()\n",
    "\n",
    "# abs_coeffs = np.abs(coeffs)\n",
    "# print(abs_coeffs)\n",
    "# print()\n",
    "\n",
    "# print(np.argsort(abs_coeffs))\n",
    "# print()\n",
    "\n",
    "# top_64_idx = np.argpartition(coeffs, -64)[-64:]\n",
    "# print(wvs[top_64_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8068b570-01ea-4bb1-8752-1e50aeb4e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GA choices\n",
    "# feats = best_pipe.named_steps['features'].best_features_\n",
    "# print(wvs[feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d927a3b-8b7f-4bf1-863e-4ea183be434f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.99113472 0.96988146 ... 0.29156386 0.29157205 0.29117255]\n",
      " [0.99113472 1.         0.99171728 ... 0.33020638 0.33037111 0.33011075]\n",
      " [0.96988146 0.99171728 1.         ... 0.36985384 0.37027856 0.3702527 ]\n",
      " ...\n",
      " [0.29156386 0.33020638 0.36985384 ... 1.         0.99951765 0.99833201]\n",
      " [0.29157205 0.33037111 0.37027856 ... 0.99951765 1.         0.99962948]\n",
      " [0.29117255 0.33011075 0.3702527  ... 0.99833201 0.99962948 1.        ]]\n",
      "\n",
      "[14 14 14 ... 13 13 13]\n",
      "\n",
      "UniqueCountsResult(values=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]), counts=array([201, 231, 317, 156,  43,  82, 101,   7,  58,  17, 413, 164, 170,\n",
      "        93,   4,  94]))\n"
     ]
    }
   ],
   "source": [
    "# # Before using permutation importance, need to correlate\n",
    "\n",
    "# corr = np.corrcoef(X.T)\n",
    "# print(corr)\n",
    "# print()\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# agg = AgglomerativeClustering(n_clusters=16)\n",
    "# clusters = agg.fit_predict(corr)\n",
    "# print(clusters)\n",
    "# print()\n",
    "# print(np.unique_counts(clusters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
