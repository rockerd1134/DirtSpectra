{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dd91d8c-be40-4c4f-b55a-2aa13f2708f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn_genetic import GAFeatureSelectionCV\n",
    "from sklearn_genetic.space import Categorical, Integer, Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c35222-7cae-4d10-8aa1-93889b0117f6",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600effed-2189-4cfa-84f8-1a317803aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether using data_consol.csv helps anything. If so, probably indicates an error in reading in or joining the separate CSVs before\n",
    "repo = git.Repo('.', search_parent_directories = True)\n",
    "root = repo.working_tree_dir\n",
    "\n",
    "data_consol = pd.read_csv(root + '//data/data_consol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d52a769-326d-4c99-ae7f-eed98a407bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_consol.filter(regex=\"^[0-9]+$\")\n",
    "bact = data_consol['pcr_bact_log']\n",
    "\n",
    "# Note: do NOT scale X and y before splitting, since that is a data leak. Instead, use the pipeline to scale both Xs, and separately scale the y for custom scoring like RMSE.\n",
    "X_train, X_test, bact_train_unscaled, bact_test_unscaled = train_test_split(X.to_numpy(), bact.to_numpy(), train_size=0.8, random_state=0)\n",
    "\n",
    "# Reshaping necessary for the y scaling step\n",
    "bact_train_unscaled = bact_train_unscaled.reshape(-1,1)\n",
    "bact_test_unscaled = bact_test_unscaled.reshape(-1,1)\n",
    "\n",
    "bact_scaler = StandardScaler()\n",
    "bact_train = bact_scaler.fit_transform(bact_train_unscaled).reshape(-1,1)\n",
    "bact_test = bact_scaler.transform(bact_test_unscaled).reshape(-1,1)\n",
    "\n",
    "# 10-fold CV; random state 0\n",
    "cv_5_0 = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80f2e2",
   "metadata": {},
   "source": [
    "**The major pipeline components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96e4672-3926-402d-b16c-85ec2caa98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the genetic algorithm feature selector\n",
    "elastic_net = ElasticNet(fit_intercept=False, warm_start=True, random_state=0, selection='random', max_iter=4000)\n",
    "\n",
    "ga_selector = GAFeatureSelectionCV(\n",
    "    estimator=elastic_net,\n",
    "    cv=cv_5_0,  # Cross-validation folds\n",
    "    scoring=\"neg_root_mean_squared_error\",  # Fitness function (maximize accuracy)\n",
    "    population_size=20,  # Number of individuals in the population\n",
    "    generations=50,  # Number of generations\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    verbose=True,  # Print progress\n",
    "    max_features = 32\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        #(\"features\", ga_selector) ,\n",
    "        (\"elastic_net\", elastic_net)\n",
    "    ], \n",
    "    memory = root+'\\\\cache',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "REGULARIZATION = np.logspace(-5, 0, 8)\n",
    "MIXTURE = np.linspace(0.001, 1, 8)\n",
    "PARAM_GRID = [\n",
    "    {\n",
    "        \"elastic_net__alpha\": REGULARIZATION,\n",
    "        \"elastic_net__l1_ratio\": MIXTURE\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(estimator=pipe, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d64d3-904a-4f44-a923-cb8ce27dd59a",
   "metadata": {},
   "source": [
    "**Train the model(s)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410ac9cb-424b-442d-af59-6f76fcb0c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid.fit(X_train, bact_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e9c61-e19d-4fa6-a301-9aeff04bbfd1",
   "metadata": {},
   "source": [
    "**Investigate results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c5b2a5-05bf-4486-9270-0bc2b0534a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Training RMSE:', round(abs(grid.score(X_train, bact_train)), 3))\n",
    "# print('Testing RMSE:', round(abs(grid.score(X_test, bact_test)), 3))\n",
    "\n",
    "# # Inverse-transforming the preds to get back to original scale.\n",
    "# # Used for comparison with R results\n",
    "# preds_unscaled = bact_scaler.inverse_transform(grid.predict(X_test).reshape(-1,1))\n",
    "# print('Testing RMSE, unscaled:', round(root_mean_squared_error(preds_unscaled, bact_test_unscaled), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c732adc1-3751-40ff-83a4-2c801bb47e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs = np.arange(350,2501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef09dedf-8f50-4c85-951d-c911328949bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter: using mutual information criterion\n",
    "# mi = mutual_info_regression(X_train, bact_train.ravel())\n",
    "# mi_top_64_idx = np.argpartition(mi, -64)[-64:]\n",
    "# print(wvs[mi_top_64_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31e5f4d2-3b6f-433f-92df-b2c8279ba4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeffs (embedded method) choices\n",
    "\n",
    "# coeffs = grid.best_estimator_['elastic_net'].coef_\n",
    "# print(coeffs)\n",
    "# print()\n",
    "\n",
    "# abs_coeffs = np.abs(coeffs)\n",
    "# print(abs_coeffs)\n",
    "# print()\n",
    "\n",
    "# print(np.argsort(abs_coeffs))\n",
    "# print()\n",
    "\n",
    "# top_64_idx = np.argpartition(coeffs, -64)[-64:]\n",
    "# print(wvs[top_64_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8068b570-01ea-4bb1-8752-1e50aeb4e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GA choices\n",
    "# feats = best_pipe.named_steps['features'].best_features_\n",
    "# print(wvs[feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92808522-e1a1-4167-842a-913c8fadcd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before using permutation importance, need to cluster based on correlation to reduce multicollinearity.\n",
    "corr = np.corrcoef(X.T) # X needs to be transposed because of corrcoef's implementation\n",
    "agg = AgglomerativeClustering(n_clusters=None, distance_threshold=0.999) # The distance threshold is somewhat arbitrary, but it's based on EDA and domain knowledge, and the results seem reasonable.\n",
    "clusters = agg.fit_predict(corr)\n",
    "\n",
    "# Now select a single \"representative\" waveband from each cluster\n",
    "cluster_choices = []\n",
    "for i in range(np.max(clusters)):\n",
    "    wv_in_cluster = wvs[clusters==i]\n",
    "    cluster_choices.append(rng.choice(wv_in_cluster))\n",
    "cluster_choices = np.sort(np.array(cluster_choices))\n",
    "cluster_choices_idx = cluster_choices-350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c1df370-de9b-450d-bf4c-dc62b470a727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.210e+01, tolerance: 3.180e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ....... (step 2 of 2) Processing elastic_net, total=   0.1s\n",
      "[1899 2285 1978 1572  717 1599 2062 2068  365 2138 1407  402 1888  991\n",
      "  450 1974  715 1882 1990 1400  529 2128 2119 2335  563 2140 1865 1887\n",
      " 2333 1226 1900 2279 1857 1959  782  921 1716 1964 1832 1396 2315  396\n",
      " 2092 1798  729 1651 2242 1372  961 1634 2020  683 1850  728 2255 1870\n",
      "  744 1415 1885  541 1847 1667 1946 1942]\n"
     ]
    }
   ],
   "source": [
    "# Build and train another elastic net model, but only on the features left after clustering, to use for permutation importance.\n",
    "pipe_perm_imp = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"elastic_net\", elastic_net)\n",
    "    ], \n",
    "    memory = root+'\\\\cache',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "grid_perm_imp = GridSearchCV(estimator=pipe_perm_imp, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "grid_perm_imp.fit(X_train[:,cluster_choices_idx], bact_train)\n",
    "perm_imp = permutation_importance(grid_perm_imp, X_train[:,cluster_choices_idx], bact_train, scoring='neg_root_mean_squared_error', n_repeats=10, n_jobs=-1, random_state=0)\n",
    "\n",
    "pi_top_64_idx = np.argpartition(perm_imp.importances_mean, -64)[-64:]\n",
    "print(cluster_choices[pi_top_64_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
