{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1c8e74-659c-4624-926c-12fcc27895b2",
   "metadata": {},
   "source": [
    "Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6dd91d8c-be40-4c4f-b55a-2aa13f2708f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "# Add more imports in this block later. There will need to be several \"from sklearn.whatever import something\" lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4168c99b-3a35-4dd1-a31c-f4daaae1880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = git.Repo('.', search_parent_directories = True)\n",
    "root = repo.working_tree_dir\n",
    "\n",
    "# The sample id and the log-transformed gene expression values.\n",
    "half_data_1 = pd.read_csv(root + '\\\\data\\\\RKNGHStress.csv')\n",
    "half_data_1 = half_data_1.loc[:, half_data_1.columns.str.startswith(('Sample', 'Log'))]\n",
    "half_data_1 = half_data_1.rename(columns = {'Sample' : 'sample', 'Log16S' : 'bact', 'Logcbblr' : 'cbblr', 'Log18S' : 'fungi', 'Logphoa' : 'phoa', 'Logurec' : 'urec'})\n",
    "\n",
    "# The hyperspectral measurements for each sample\n",
    "half_data_2 = pd.read_csv(root + '\\\\data\\\\RKNGHStressPCAPSR.csv')\n",
    "half_data_2 = half_data_2.rename(columns = {'Unnamed: 0' : 'sample'})\n",
    "\n",
    "data = half_data_1.join(half_data_2.set_index('sample'), on = 'sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d2cc1-a33c-43bc-b1dd-72b2ed921d26",
   "metadata": {},
   "source": [
    "TEMP: testing manual construction of models with specific hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "590160bb-bbbd-4c7a-bbba-4b892c5ac65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['sample', 'bact', 'cbblr', 'fungi', 'phoa', 'urec'], axis = 1)\n",
    "bact = data[['bact']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "389c2476-08f3-419f-8f01-9be461b04156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-471.04533586184095"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, bact_train, bact_test = train_test_split(X.to_numpy(), bact.to_numpy(), train_size = 0.8)\n",
    "bact_train = bact_train.ravel()\n",
    "bact_test = bact_test.ravel()\n",
    "\n",
    "# Note: do NOT scale X and y before splitting, since that is a data leak. Instead, use the pipeline to scale both Xs and the y training, and manually scale the y testing for custom scoring like RMSE.\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(), ElasticNetCV(alphas = [0.001389495], l1_ratio = 0.4285714, cv = 10, selection = 'random', max_iter = 10000))\n",
    "pipeline.fit(X_train, bact_train)\n",
    "pipeline.score(X_test, bact_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f46efa-9605-446e-bc1d-91afb144bbe6",
   "metadata": {},
   "source": [
    "This looks weird. None of the folds are converging (true for 10 folds, 5 folds, or even just 1 fold with basic ElasticNet) but changing selection to 'random' and max_iter to 10k allowed convergence. \n",
    "\n",
    "The score is pretty low. But what metric is the score? If it's not RMSE it's not in the ballpark of the R version. (UPDATE: It's R^2, so it's a bunch of pretty bad scores, actually.)\n",
    "\n",
    "It looks like ElasticNetCV, and ElasticNet for that matter, don't allow changing the scoring or tuning metric, or at least it's not obvious how. Can the models be evaluated on RMSE just by calling the RMSE function on the preds and targets? Also, does it even make sense to change the tuning metric for this algorithm? (Idk, you probably COULD, but minimizing the sum of squared residuals is good enough.) And what tuning metric does the tidymodels implementation use?\n",
    "\n",
    "This all might be caused by using the hyperparameter optima found in the R code, but that had a differet train/test split. So it's not optimal here, but it's still probably pretty good, especially since in the analysis of part 1's hyperparameters, there wasn't much variation among the elastic net models' penalties (all tending to be very close to 0) or mixtures (a bit more spread but similar).\n",
    "\n",
    "What happens if some of its own tuning were allowed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c828750-f67b-460c-b8d8-94fd1b31a5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.30167882 -0.32435681 -0.16994891  0.39670121  0.25491271 -0.1838957\n",
      " -0.35130211  0.18132449  0.11240183  0.21245123 -0.28431435 -0.10937956\n",
      "  0.02726354  0.34610739 -0.08573751 -0.12437335 -0.44832629  0.43490646\n",
      " -0.2331579  -0.03646126  0.29497726  0.08395952  0.14403236 -0.32314219\n",
      " -0.13073932  0.3601685   0.3271546   0.05173241 -0.08478912 -0.18042365\n",
      "  0.28732325 -0.55372755  0.26427954  0.10760681 -0.12250387  0.53005803\n",
      " -0.27401899  0.58678425  0.42258126 -0.32610505  0.26361617 -0.03611554\n",
      "  0.47091699 -0.32362347  0.23242506 -0.20872366 -0.35041646  0.09218657\n",
      " -0.26156427  0.36010758 -0.03356407  0.44726391  0.53851241  0.05622336\n",
      " -0.27942402  0.15987401 -0.2200759   0.30743585 -0.26980634  0.53244131\n",
      " -0.15480568  0.2872557  -0.17786029 -0.00411791 -0.28753649 -0.4676467\n",
      " -0.17377226 -0.37650008  0.08047002  0.37637611 -0.21343528  0.29685338\n",
      " -0.34337752 -0.16002197 -0.23365526 -0.50311291  0.11201752  0.28377126\n",
      "  0.19557695 -0.10578414]\n",
      "\n",
      "[ 0.22532478 -1.15687247  0.83377282  1.16781008  1.28974396 -1.45289762\n",
      " -1.92192367  0.88511289  1.4581519   0.61459382 -1.0277049  -0.49507427\n",
      " -0.83424925  1.40021712 -0.60817929 -0.89017864 -1.1249385   1.48287945\n",
      "  0.22532478 -1.08560055  0.48309007  1.4581519   1.05298207  0.22532478\n",
      " -1.08560055  0.98187122  0.75681935 -0.97820577 -1.42119648 -0.61557019\n",
      "  0.90374448 -1.08458957  0.48309007  0.72760045 -0.28567844  1.48287945\n",
      "  0.50924899  0.50103545  0.5265076  -0.61557019  0.56195021 -1.21147355\n",
      "  1.00185769 -1.00654491  1.40022825 -0.96784943 -1.34050014 -0.96962852\n",
      " -1.34050014  0.98187122 -0.94204876  0.37369194  0.78835412 -1.30993241\n",
      " -0.60817929  1.11183264 -0.85501182  1.48287945  0.83377282  0.03435567\n",
      "  1.4581519   0.90374448 -0.92544812  0.56195021 -1.42119648 -1.15921952\n",
      " -1.10760509 -0.89608939  0.72760045  0.50103545 -0.42699032  0.90374448\n",
      " -0.94204876 -1.45289762 -1.08560055 -0.36760164  0.85166489  0.5265076\n",
      "  1.00185769  1.33806813]\n",
      "\n",
      "0.831831038314275\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.predict(X_test))\n",
    "print()\n",
    "print(scale(bact_test))\n",
    "print()\n",
    "# RMSE since squared = False\n",
    "rmse = mse(scale(bact_test), pipeline.predict(X_test), squared = False)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23386e86-b543-4d9a-943f-dd7f622db698",
   "metadata": {},
   "source": [
    "Uh oh, RMSE of 0.83 is really bad. It doesn't even come close to the R models. What happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aaf973-4ffc-48af-a0dd-f0534194228a",
   "metadata": {},
   "source": [
    "Now try to get the distance filter up and running. This is subtly different than the correlation filter, which is probably too heavy-handed on this data. But the other aspect to consider is that the distance filter can't be applied during preprocessing (before building models) like the correlation filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e466a7-4046-4fd9-82f5-1044b6d32d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
