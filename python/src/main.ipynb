{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd91d8c-be40-4c4f-b55a-2aa13f2708f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn_genetic import GAFeatureSelectionCV\n",
    "from sklearn_genetic.space import Categorical, Integer, Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600effed-2189-4cfa-84f8-1a317803aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether using data_consol.csv helps anything. If so, probably indicates an error in reading in or joining the separate CSVs before\n",
    "repo = git.Repo('.', search_parent_directories = True)\n",
    "root = repo.working_tree_dir\n",
    "\n",
    "data_consol = pd.read_csv(root + '//data/data_consol.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d52a769-326d-4c99-ae7f-eed98a407bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_consol.filter(regex=\"^[0-9]+$\")\n",
    "bact = data_consol['pcr_bact_log']\n",
    "\n",
    "# Note: do NOT scale X and y before splitting, since that is a data leak. Instead, use the pipeline to scale both Xs, and separately scale the y for custom scoring like RMSE.\n",
    "X_train, X_test, bact_train_unscaled, bact_test_unscaled = train_test_split(X.to_numpy(), bact.to_numpy(), train_size=0.8, random_state=0)\n",
    "\n",
    "# Reshaping necessary for the y scaling step\n",
    "bact_train_unscaled = bact_train_unscaled.reshape(-1,1)\n",
    "bact_test_unscaled = bact_test_unscaled.reshape(-1,1)\n",
    "\n",
    "bact_scaler = StandardScaler()\n",
    "bact_train = bact_scaler.fit_transform(bact_train_unscaled)\n",
    "bact_test = bact_scaler.transform(bact_test_unscaled)\n",
    "\n",
    "# 10-fold CV; random state 0\n",
    "cv_10_0 = KFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f496c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElasticNet(fit_intercept=False, warm_start=True, random_state=0, selection='random', max_iter=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80f2e2",
   "metadata": {},
   "source": [
    "\n",
    "Now let's set up the feature selector object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffdff91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the genetic algorithm feature selector\n",
    "selector = GAFeatureSelectionCV(\n",
    "    estimator=model,\n",
    "    cv=10,  # Cross-validation folds\n",
    "    scoring=\"neg_root_mean_squared_error\",  # Fitness function (maximize accuracy)\n",
    "    population_size=20,  # Number of individuals in the population\n",
    "    max_features=300,\n",
    "    generations=50,  # Number of generations\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    verbose=False,  # Print progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924b9cd-f5f9-4d24-9c73-0e43a3a21b0e",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"features\", selector ),\n",
    "        (\"elastic_net\",  model )\n",
    "    ], \n",
    "    memory = root+'\\\\cache',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "REGULARIZATION = np.logspace(-5, 0, 8)\n",
    "MIXTURE = np.linspace(0.001, 1, 8)\n",
    "PARAM_GRID = [\n",
    "    {\n",
    "        \"elastic_net__alpha\": REGULARIZATION,\n",
    "        \"elastic_net__l1_ratio\": MIXTURE\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(estimator=pipe, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_10_0, error_score='raise')\n",
    "grid.fit(X_train, bact_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c9d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training RMSE:', round(abs(grid.score(X_train, bact_train)), 3))\n",
    "print('Testing RMSE:', round(abs(grid.score(X_test, bact_test)), 3))\n",
    "\n",
    "# Inverse-transforming the preds to get back to original scale.\n",
    "# Used for comparison with R results\n",
    "preds_unscaled = bact_scaler.inverse_transform(grid.predict(X_test).reshape(-1,1))\n",
    "print('Testing RMSE, unscaled:', round(root_mean_squared_error(preds_unscaled, bact_test_unscaled), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5b2a5-05bf-4486-9270-0bc2b0534a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training RMSE:', round(abs(grid.score(X_train, bact_train)), 3))\n",
    "print('Testing RMSE:', round(abs(grid.score(X_test, bact_test)), 3))\n",
    "\n",
    "# Inverse-transforming the preds to get back to original scale.\n",
    "# Used for comparison with R results\n",
    "preds_unscaled = bact_scaler.inverse_transform(grid.predict(X_test).reshape(-1,1))\n",
    "print('Testing RMSE, unscaled:', round(root_mean_squared_error(preds_unscaled, bact_test_unscaled), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe = grid.best_estimator_\n",
    "selector = best_pipe.named_steps['features']\n",
    "\n",
    "# Check if the selector has the 'best_features_' attribute\n",
    "if hasattr(selector, \"best_features_\"):\n",
    "    # Get the mask of selected features (True for selected, False for not selected)\n",
    "    selected_features_mask = selector.best_features_\n",
    "\n",
    "    # Get the feature names (if available)\n",
    "    feature_names = X_train.columns if hasattr(X_train, \"columns\") else [f\"Feature_{i}\" for i in range(X_train.shape[1])]\n",
    "    selected_feature_names = [name for name, selected in zip(feature_names, selected_features_mask) if selected]\n",
    "\n",
    "    print(f\"Selected Features Count: {len(selected_feature_names)}\")\n",
    "    print(\"Selected Features:\", selected_feature_names)\n",
    "else:\n",
    "    print(\"The attribute 'best_features_' is not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18c2d8-000c-411a-838a-59f83c3ca5fa",
   "metadata": {},
   "source": [
    "*Testing models' results:*\n",
    "\n",
    "For elastic_net random_state=0, training RMSE is 0.587, and testing RMSE is 0.766.\n",
    "\n",
    "After removing the scaler, training RMSE is 0.491, and testing RMSE is 0.629\n",
    "\n",
    "No scaler; optimizing R2: training RMSE is 0.236, testing RMSE is 0.935\n",
    "\n",
    "No scaler; 10-fold CV: training RMSE 0.491, testing RMSE 0.629\n",
    "\n",
    "With scaler; 10-fold CV; unscaling preds: training RMSE 0.587, testing RMSE 0.338 (on par with R results)\n",
    "\n",
    "*Final model's results:*\n",
    "\n",
    "Training RMSE: 0.587\n",
    "\n",
    "Testing RMSE: 0.766\n",
    "\n",
    "Testing RMSE, unscaled: 0.338 (on par with R results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
