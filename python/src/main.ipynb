{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd91d8c-be40-4c4f-b55a-2aa13f2708f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn_genetic import GAFeatureSelectionCV\n",
    "# from sklearn_genetic.space import Categorical, Integer, Continuous\n",
    "\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c35222-7cae-4d10-8aa1-93889b0117f6",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600effed-2189-4cfa-84f8-1a317803aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether using data_consol.csv helps anything. If so, probably indicates an error in reading in or joining the separate CSVs before\n",
    "repo = git.Repo('.', search_parent_directories = True)\n",
    "root = repo.working_tree_dir\n",
    "\n",
    "data_consol = pd.read_csv(root + '//data/data_consol.csv')\n",
    "\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d52a769-326d-4c99-ae7f-eed98a407bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_consol.filter(regex=\"^[0-9]+$\")\n",
    "bact = data_consol['pcr_bact_log']\n",
    "\n",
    "# Note: do NOT scale X and y before splitting, since that is a data leak. Instead, use the pipeline to scale both Xs, and separately scale the y for custom scoring like RMSE.\n",
    "X_train, X_test, bact_train_unscaled, bact_test_unscaled = train_test_split(X.to_numpy(), bact.to_numpy(), train_size=0.8, random_state=0)\n",
    "\n",
    "# Reshaping necessary for the y scaling step\n",
    "bact_train_unscaled = bact_train_unscaled.reshape(-1,1)\n",
    "bact_test_unscaled = bact_test_unscaled.reshape(-1,1)\n",
    "\n",
    "bact_scaler = StandardScaler()\n",
    "bact_train = bact_scaler.fit_transform(bact_train_unscaled).reshape(-1,1)\n",
    "bact_test = bact_scaler.transform(bact_test_unscaled).reshape(-1,1)\n",
    "\n",
    "# 5-fold CV; random state 0\n",
    "cv_5_0 = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Used for waveband selection\n",
    "wvs = np.arange(350,2501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92808522-e1a1-4167-842a-913c8fadcd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is only with respect to X_train, not any of the target variables, this only has to be computed once. (It's relatively cheap to compute, but this also has the benefit of preserving the random choices.)\n",
    "def cluster(X_train):\n",
    "    \"\"\" Uses agglomerative clustering with a distance threshold of 0.999 on the normalized feature correlation coefficient matrix. Then, it randomly selects one waveband from each cluster.\n",
    "    This should be used as a preprocessing step when doing permutation importance. (Clustering method) \"\"\"\n",
    "    corr = np.corrcoef(X.T) # X needs to be transposed because of corrcoef's implementation\n",
    "    agg = AgglomerativeClustering(n_clusters=None, distance_threshold=0.999) # The distance threshold is somewhat arbitrary, but it's based on EDA and domain knowledge, and the results seem reasonable.\n",
    "    clusters = agg.fit_predict(corr)\n",
    "    # Now select a single \"representative\" waveband from each cluster\n",
    "    cluster_choices = []\n",
    "    for i in range(np.max(clusters)):\n",
    "        wv_in_cluster = wvs[clusters==i]\n",
    "        cluster_choices.append(rng.choice(wv_in_cluster))\n",
    "    cluster_choices = np.sort(np.array(cluster_choices))\n",
    "    return cluster_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e43ac874-fce3-4b88-b42e-00897bae084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_choices = cluster(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80f2e2",
   "metadata": {},
   "source": [
    "**The major pipeline components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96e4672-3926-402d-b16c-85ec2caa98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet(fit_intercept=False, warm_start=True, random_state=0, selection='random', max_iter=4000)\n",
    "\n",
    "# Used for embedded feature importance (via coeffs) and wrapper feature importance (via perm importance)\n",
    "pipe_elastic_net = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"elastic_net\", elastic_net)\n",
    "    ],\n",
    "    memory = root+'\\\\cache',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Hyperparameters for elastic net tuning. When code is finalized, expand for more thorough search using more computational resources.\n",
    "REGULARIZATION = np.logspace(-5, 0, 8)\n",
    "MIXTURE = np.linspace(0.001, 1, 8)\n",
    "PARAM_GRID = [\n",
    "    {\n",
    "        \"elastic_net__alpha\": REGULARIZATION,\n",
    "        \"elastic_net__l1_ratio\": MIXTURE\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d64d3-904a-4f44-a923-cb8ce27dd59a",
   "metadata": {},
   "source": [
    "**Feature selection functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef09dedf-8f50-4c85-951d-c911328949bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi(X_train, y_train, n_features=64):\n",
    "    \"\"\" Uses mutual information to calculate the n_features most related features in X_train to y_train. (Filter method) \"\"\"\n",
    "    y_train = y_train.ravel()\n",
    "    mi = mutual_info_regression(X_train, y_train)\n",
    "    top_n_idx = np.argpartition(mi, -n_features)[-n_features:]\n",
    "    return wvs[top_n_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e5f4d2-3b6f-433f-92df-b2c8279ba4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeffs(X_train, y_train, n_features=64):\n",
    "    \"\"\" Builds and fits an elastic net model using all features. Returns the n_features features with the highest absolute-valued coefficients. (Embedded method) \"\"\"\n",
    "    grid = GridSearchCV(estimator=pipe_elastic_net, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "    grid.fit(X_train, y_train)\n",
    "    coeffs = grid.best_estimator_['elastic_net'].coef_\n",
    "    abs_coeffs = np.abs(coeffs)\n",
    "    top_n_idx = np.argpartition(abs_coeffs, -n_features)[-n_features:]\n",
    "    return wvs[top_n_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "693601be-723e-4ce0-b1a5-676fbdd59413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ga(X_train, y_train, n_features=64):\n",
    "    \"\"\" Uses a genetic algorithm to find the wavebands that gives the lowest RMSE on an elastic net model. \n",
    "    The subset will be at most n_features large, but it may be less than n_features large. (GA method) \"\"\"\n",
    "    y_train = y_train.ravel()\n",
    "    ga_selector = GAFeatureSelectionCV(\n",
    "        estimator=elastic_net,\n",
    "        cv=cv_5_0,  # Cross-validation folds\n",
    "        scoring=\"neg_root_mean_squared_error\",  # Fitness function (maximize accuracy)\n",
    "        population_size=ceil(n_features*1.5),  # Number of individuals in the population\n",
    "        generations=10,  # Number of generations\n",
    "        n_jobs=-1,  # Use all available CPU cores\n",
    "        verbose=True,  # Print progress\n",
    "        max_features=n_features\n",
    "    )\n",
    "    pipe_ga = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"ga\", ga_selector)\n",
    "        ], \n",
    "        memory = root+'\\\\cache',\n",
    "        verbose=True\n",
    "    )\n",
    "    pipe_ga.fit(X_train, y_train)\n",
    "    feats = pipe_ga['ga'].best_features_\n",
    "    return wvs[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78bd175-1cf6-4533-a305-dacce062f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_imp(X_train, y_train, n_features=64):\n",
    "    \"\"\" Calculates permutation importance on a dataset. cluster_choices should be the result of calling cluster(), which should be done once at the start of execution. \n",
    "    This is done outside this function to preserve the random selection. Returns the set of n_features wavebands with the highest permutation importance on the training set. (Wrapper method) \"\"\"\n",
    "    # Use only the features selected by clustering\n",
    "    cluster_idx = cluster_choices - 350\n",
    "    X_train = X_train[:,cluster_idx]\n",
    "    # Build and train another elastic net model, but only on the features left after clustering, to use for permutation importance.\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"elastic_net\", elastic_net)\n",
    "        ], \n",
    "        memory = root+'\\\\cache',\n",
    "        verbose=True\n",
    "    )    \n",
    "    grid = GridSearchCV(estimator=pipe, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "    grid.fit(X_train, y_train)\n",
    "    perm_imp = permutation_importance(grid, X_train, y_train, scoring='neg_root_mean_squared_error', n_repeats=10, n_jobs=-1, random_state=0)\n",
    "    pi_top_64_idx = np.argpartition(perm_imp.importances_mean, -64)[-64:]\n",
    "    return cluster_choices[pi_top_64_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465453b6-7b18-40c6-b88a-502d1a9f4c9e",
   "metadata": {},
   "source": [
    "**Consensus function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34ec3e1d-194a-4195-a1e8-99d9deb55032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP: a holder for wv_intermed so the rest doesn't have to be recalculated while debugging\n",
    "temp_wv_intermed = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed9401-293e-4afd-82e3-45e57f1bccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus(X_train, y_train, n_features_intermed=64, max_features_output=8):\n",
    "    \"\"\" Takes the wavebands output by the feature selection functions and uses a (separate) genetic algorithm to find the wavebands that give the lowest RMSE on an elastic net model.\n",
    "    The subset will be at most n_features large, but it may be less than n_features large. (Consensus method) \"\"\"\n",
    "    \n",
    "    # TODO: implement caching check like in R to avoid repeated computations\n",
    "    # print('\\tFEATURE SELECTION:')\n",
    "    # print('\\tStarting mutual importance...', end=' ')\n",
    "    # wv_mi = mi(X_train, y_train, n_features=n_features_intermed)\n",
    "    # print('Done.')\n",
    "    # print('\\tStarting linear coefficients...', end=' ')\n",
    "    # wv_coeffs = coeffs(X_train, y_train, n_features=n_features_intermed)\n",
    "    # print('Done.')\n",
    "    # print('\\tStarting genetic algorithm...', end=' ')\n",
    "    # wv_ga = ga(X_train, y_train, n_features=n_features_intermed)\n",
    "    # print('Done.')\n",
    "    # print('\\tStarting permutation importance...', end=' ')\n",
    "    # wv_cluster = cluster_choices # Doesn't require a separate function call\n",
    "    # wv_perm_imp = perm_imp(X_train, y_train, n_features=n_features_intermed)\n",
    "    # print('Done.')\n",
    "\n",
    "    # # Compile the above results into one array, remove any duplicates, and sort.\n",
    "    # wv_intermed = np.append(wv_mi, wv_coeffs)\n",
    "    # wv_intermed = np.append(wv_intermed, wv_ga)\n",
    "    # wv_intermed = np.append(wv_intermed, wv_cluster)\n",
    "    # wv_intermed = np.append(wv_intermed, wv_perm_imp)\n",
    "    # wv_intermed = np.sort(np.unique(wv_intermed))\n",
    "\n",
    "    # TEMP: (For commenting out back and forth)\n",
    "    temp_wv_intermed = wv_intermed\n",
    "    wv_intermed = temp_wv_intermed\n",
    "\n",
    "    # Convert the above into indices for masking over the dataset.\n",
    "    wv_intermed_idx = wv_intermed-350\n",
    "    X_train = X_train[:,wv_intermed_idx]\n",
    "\n",
    "    # Use another genetic algorithm to find the best wavebands out of the narrowed possibilities\n",
    "    print('\\tCONSENSUS:')\n",
    "    print('\\tStarting genetic algorithm...', end=' ')\n",
    "    wv_consensus = ga(X_train, y_train, n_features=max_features_output)\n",
    "    print('\\tDone.')\n",
    "    return wv_consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c060562f-435e-4a1a-806c-51e1f4183677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCONSENSUS:\n",
      "\tStarting genetic algorithm... [Pipeline] ............ (step 1 of 2) Processing scaler, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tfitness \tfitness_std\tfitness_max\tfitness_min\n",
      "0  \t12    \t-50000.5\t49999.5    \t-0.99933   \t-100000    \n",
      "1  \t24    \t-41667.2\t49300.2    \t-0.99933   \t-100000    \n",
      "2  \t24    \t-8334.25\t27638.3    \t-0.99933   \t-100000    \n",
      "3  \t24    \t-16667.5\t37267.4    \t-0.99933   \t-100000    \n",
      "4  \t24    \t-25000.7\t43300.8    \t-0.99933   \t-100000    \n",
      "5  \t24    \t-16667.5\t37267.4    \t-0.99933   \t-100000    \n",
      "6  \t24    \t-8334.25\t27638.3    \t-0.99933   \t-100000    \n",
      "7  \t24    \t-25000.7\t43300.8    \t-0.99933   \t-100000    \n",
      "8  \t24    \t-41667.2\t49300.2    \t-0.99933   \t-100000    \n",
      "9  \t24    \t-33334  \t47140      \t-0.99933   \t-100000    \n",
      "10 \t24    \t-33334  \t47140      \t-0.99933   \t-100000    \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (33,) into shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m temp \u001b[38;5;241m=\u001b[39m consensus(X_train, bact_train)\n",
      "Cell \u001b[1;32mIn[22], line 38\u001b[0m, in \u001b[0;36mconsensus\u001b[1;34m(X_train, y_train, n_features_intermed, max_features_output)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mCONSENSUS:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mStarting genetic algorithm...\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m wv_consensus \u001b[38;5;241m=\u001b[39m ga(X_train, y_train, n_features\u001b[38;5;241m=\u001b[39mmax_features_output)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wv_consensus\n",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m, in \u001b[0;36mga\u001b[1;34m(X_train, y_train, n_features)\u001b[0m\n\u001b[0;32m      5\u001b[0m ga_selector \u001b[38;5;241m=\u001b[39m GAFeatureSelectionCV(\n\u001b[0;32m      6\u001b[0m     estimator\u001b[38;5;241m=\u001b[39melastic_net,\n\u001b[0;32m      7\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv_5_0,  \u001b[38;5;66;03m# Cross-validation folds\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     max_features\u001b[38;5;241m=\u001b[39mn_features\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m pipe_ga \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[0;32m     16\u001b[0m     [\n\u001b[0;32m     17\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m, StandardScaler()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m pipe_ga\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     24\u001b[0m feats \u001b[38;5;241m=\u001b[39m pipe_ga[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mga\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbest_features_\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wvs[feats]\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn_genetic\\genetic_search.py:1181\u001b[0m, in \u001b[0;36mGAFeatureSelectionCV.fit\u001b[1;34m(self, X, y, callbacks)\u001b[0m\n\u001b[0;32m   1178\u001b[0m bool_individual \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_features_, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m   1180\u001b[0m refit_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 1181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_[:, bool_individual], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_)\n\u001b[0;32m   1182\u001b[0m refit_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit_time_ \u001b[38;5;241m=\u001b[39m refit_end_time \u001b[38;5;241m-\u001b[39m refit_start_time\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1077\u001b[0m, in \u001b[0;36mElasticNet.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1076\u001b[0m     this_Xy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1077\u001b[0m _, this_coef, this_dual_gap, this_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath(\n\u001b[0;32m   1078\u001b[0m     X,\n\u001b[0;32m   1079\u001b[0m     y[:, k],\n\u001b[0;32m   1080\u001b[0m     l1_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_ratio,\n\u001b[0;32m   1081\u001b[0m     eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1082\u001b[0m     n_alphas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1083\u001b[0m     alphas\u001b[38;5;241m=\u001b[39m[alpha],\n\u001b[0;32m   1084\u001b[0m     precompute\u001b[38;5;241m=\u001b[39mprecompute,\n\u001b[0;32m   1085\u001b[0m     Xy\u001b[38;5;241m=\u001b[39mthis_Xy,\n\u001b[0;32m   1086\u001b[0m     copy_X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1087\u001b[0m     coef_init\u001b[38;5;241m=\u001b[39mcoef_[k],\n\u001b[0;32m   1088\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1089\u001b[0m     return_n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1090\u001b[0m     positive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive,\n\u001b[0;32m   1091\u001b[0m     check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# from here on **params\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m   1094\u001b[0m     X_offset\u001b[38;5;241m=\u001b[39mX_offset,\n\u001b[0;32m   1095\u001b[0m     X_scale\u001b[38;5;241m=\u001b[39mX_scale,\n\u001b[0;32m   1096\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[0;32m   1097\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[0;32m   1098\u001b[0m     selection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselection,\n\u001b[0;32m   1099\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1100\u001b[0m )\n\u001b[0;32m   1101\u001b[0m coef_[k] \u001b[38;5;241m=\u001b[39m this_coef[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1102\u001b[0m dual_gaps_[k] \u001b[38;5;241m=\u001b[39m this_dual_gap[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:706\u001b[0m, in \u001b[0;36menet_path\u001b[1;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecompute should be one of True, False, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or array-like. Got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;241m%\u001b[39m precompute\n\u001b[0;32m    704\u001b[0m     )\n\u001b[0;32m    705\u001b[0m coef_, dual_gap_, eps_, n_iter_ \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 706\u001b[0m coefs[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, i] \u001b[38;5;241m=\u001b[39m coef_\n\u001b[0;32m    707\u001b[0m \u001b[38;5;66;03m# we correct the scale of the returned dual gap, as the objective\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;66;03m# in cd_fast is n_samples * the objective in this docstring.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m dual_gaps[i] \u001b[38;5;241m=\u001b[39m dual_gap_ \u001b[38;5;241m/\u001b[39m n_samples\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (33,) into shape (2,)"
     ]
    }
   ],
   "source": [
    "wv_consensus = consensus(X_train, bact_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdc7a0d4-ca45-46aa-8aaa-2d5140e1b240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 350  351  352  353  354  355  356  357  358  359  360  361  362  363\n",
      "  364  365  366  367  370  373  382  383  385  391  394  395  396  397\n",
      "  401  402  406  407  409  421  428  429  430  450  457  472  473  474\n",
      "  475  476  477  478  479  489  496  498  511  512  514  519  525  531\n",
      "  540  549  562  566  581  603  615  632  642  647  653  659  660  663\n",
      "  666  667  678  680  690  691  693  694  697  700  707  708  710  712\n",
      "  715  716  717  718  719  720  721  724  726  727  730  736  742  747\n",
      "  780  782  800  803  831  852  886  891  900  925  959  962  971  973\n",
      "  974  975  976  977  978  979  980  981  983  984  985 1062 1093 1109\n",
      " 1150 1203 1218 1247 1282 1321 1333 1344 1361 1365 1380 1382 1386 1391\n",
      " 1395 1398 1405 1410 1430 1436 1437 1442 1443 1444 1445 1450 1482 1483\n",
      " 1486 1490 1499 1507 1509 1514 1521 1523 1531 1539 1553 1567 1577 1600\n",
      " 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634\n",
      " 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1665 1696\n",
      " 1699 1705 1714 1715 1722 1746 1750 1769 1786 1823 1827 1845 1849 1853\n",
      " 1857 1862 1867 1868 1869 1870 1871 1873 1877 1880 1883 1884 1885 1886\n",
      " 1887 1888 1889 1890 1892 1895 1896 1898 1899 1900 1901 1902 1903 1904\n",
      " 1911 1913 1916 1917 1920 1925 1930 1934 1935 1936 1937 1938 1939 1940\n",
      " 1941 1942 1943 1944 1945 1946 1947 1948 1954 1957 1969 1975 1983 1987\n",
      " 2004 2005 2006 2007 2008 2018 2025 2044 2051 2057 2082 2091 2109 2117\n",
      " 2120 2128 2133 2139 2150 2155 2171 2179 2196 2240 2247 2253 2254 2255\n",
      " 2256 2257 2258 2259 2260 2261 2262 2270 2280 2294 2308 2312 2313 2314\n",
      " 2330 2333 2334 2347 2348 2349 2350 2361 2379 2384 2409 2411 2429 2432\n",
      " 2435 2456 2465 2471 2482 2491]\n",
      "(328,)\n",
      "(318, 328)\n"
     ]
    }
   ],
   "source": [
    "# print(test)\n",
    "# print(test.shape)\n",
    "\n",
    "# test_idx = test - 350\n",
    "# print(X_train[:,test_idx].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e9c61-e19d-4fa6-a301-9aeff04bbfd1",
   "metadata": {},
   "source": [
    "**Investigate results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94c5b2a5-05bf-4486-9270-0bc2b0534a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Training RMSE:', round(abs(elastic_net_grid.score(X_train, bact_train)), 3))\n",
    "# print('Testing RMSE:', round(abs(elastic_net_grid.score(X_test, bact_test)), 3))\n",
    "\n",
    "# # Inverse-transforming the preds to get back to original scale.\n",
    "# # Used for comparison with R results\n",
    "# preds_unscaled = bact_scaler.inverse_transform(elastic_net_grid.predict(X_test).reshape(-1,1))\n",
    "# print('Testing RMSE, unscaled:', round(root_mean_squared_error(preds_unscaled, bact_test_unscaled), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
