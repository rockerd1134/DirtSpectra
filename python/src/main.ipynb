{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd91d8c-be40-4c4f-b55a-2aa13f2708f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn_genetic import GAFeatureSelectionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c35222-7cae-4d10-8aa1-93889b0117f6",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d6faae-0195-426f-90d5-db5b48f055dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing whether using data_consol.csv helps anything. If so, probably indicates an error in reading in or joining the separate CSVs before\n",
    "repo = git.Repo('.', search_parent_directories = True)\n",
    "root = repo.working_tree_dir\n",
    "\n",
    "data_consol = pd.read_csv(root + '//data/data_consol.csv')\n",
    "\n",
    "SEED = 0\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# Intended for reproducible GA steps\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "X = data_consol.filter(regex=\"^[0-9]+$\")\n",
    "    \n",
    "# Note: do NOT scale X and y before splitting, since that is a data leak. Instead, use the pipeline to scale both Xs, and separately scale the y for custom scoring like RMSE.\n",
    "X_train, X_test, y_train_unscaled, y_test_unscaled = train_test_split(data_consol.filter(regex=\"^[0-9]+$\").to_numpy(), data_consol.filter(regex=\"pcr_[a-z]+_log\"), train_size=0.8, random_state=0)\n",
    "\n",
    "# Separate y by genes. Reshaping necessary for the y scaling step\n",
    "bact_train_unscaled = y_train_unscaled[\"pcr_bact_log\"].to_numpy().reshape(-1,1)\n",
    "bact_test_unscaled = y_test_unscaled[\"pcr_bact_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "cbblr_train_unscaled = y_train_unscaled[\"pcr_cbblr_log\"].to_numpy().reshape(-1,1)\n",
    "cbblr_test_unscaled = y_test_unscaled[\"pcr_cbblr_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "fungi_train_unscaled = y_train_unscaled[\"pcr_fungi_log\"].to_numpy().reshape(-1,1)\n",
    "fungi_test_unscaled = y_test_unscaled[\"pcr_fungi_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "urec_train_unscaled = y_train_unscaled[\"pcr_urec_log\"].to_numpy().reshape(-1,1)\n",
    "urec_test_unscaled = y_test_unscaled[\"pcr_urec_log\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "# Special case: phoa has 10 NAN rows that need to be removed from both its X and y.\n",
    "phoa_data = data_consol.filter(regex=\"^[0-9]+$|pcr_phoa_log\").dropna()\n",
    "X_phoa = phoa_data.to_numpy()[:,:2151]\n",
    "phoa = phoa_data[\"pcr_phoa_log\"].to_numpy()\n",
    "X_phoa_train, X_phoa_test, phoa_train_unscaled, phoa_test_unscaled = train_test_split(X_phoa, phoa, train_size=0.8, random_state=0)\n",
    "phoa_train_unscaled = phoa_train_unscaled.reshape(-1,1)\n",
    "phoa_test_unscaled = phoa_test_unscaled.reshape(-1,1)\n",
    "\n",
    "# Scale each y with respect to its distribution\n",
    "\n",
    "bact_scaler = StandardScaler()\n",
    "bact_train = bact_scaler.fit_transform(bact_train_unscaled).reshape(-1,1)\n",
    "bact_test = bact_scaler.transform(bact_test_unscaled).reshape(-1,1)\n",
    "\n",
    "cbblr_scaler = StandardScaler()\n",
    "cbblr_train = cbblr_scaler.fit_transform(cbblr_train_unscaled).reshape(-1,1)\n",
    "cbblr_test = cbblr_scaler.transform(cbblr_test_unscaled).reshape(-1,1)\n",
    "\n",
    "fungi_scaler = StandardScaler()\n",
    "fungi_train = fungi_scaler.fit_transform(fungi_train_unscaled).reshape(-1,1)\n",
    "fungi_test = fungi_scaler.transform(fungi_test_unscaled).reshape(-1,1)\n",
    "\n",
    "phoa_scaler = StandardScaler()\n",
    "phoa_train = phoa_scaler.fit_transform(phoa_train_unscaled).reshape(-1,1)\n",
    "phoa_test = phoa_scaler.transform(phoa_test_unscaled).reshape(-1,1)\n",
    "\n",
    "urec_scaler = StandardScaler()\n",
    "urec_train = urec_scaler.fit_transform(urec_train_unscaled).reshape(-1,1)\n",
    "urec_test = urec_scaler.transform(urec_test_unscaled).reshape(-1,1)\n",
    "\n",
    "# 5-fold CV; random state 0\n",
    "cv_5_0 = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Used for waveband selection\n",
    "wvs = np.arange(350,2501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92808522-e1a1-4167-842a-913c8fadcd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is only with respect to X_train, not any of the target variables, this only has to be computed once. (It's relatively cheap to compute, but this also has the benefit of preserving the random choices.)\n",
    "def cluster(X_train):\n",
    "    \"\"\" Uses agglomerative clustering with a distance threshold of 0.999 on the normalized feature correlation coefficient matrix. Then, it randomly selects one waveband from each cluster.\n",
    "    This should be used as a preprocessing step when doing permutation importance. (Clustering method) \"\"\"\n",
    "    corr = np.corrcoef(X.T) # X needs to be transposed because of corrcoef's implementation\n",
    "    agg = AgglomerativeClustering(n_clusters=None, distance_threshold=0.999) # The distance threshold is somewhat arbitrary, but it's based on EDA and domain knowledge, and the results seem reasonable.\n",
    "    clusters = agg.fit_predict(corr)\n",
    "    # Now select a single \"representative\" waveband from each cluster\n",
    "    cluster_choices = []\n",
    "    for i in range(np.max(clusters)):\n",
    "        wv_in_cluster = wvs[clusters==i]\n",
    "        cluster_choices.append(rng.choice(wv_in_cluster))\n",
    "    cluster_choices = np.sort(np.array(cluster_choices))\n",
    "    return cluster_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43ac874-fce3-4b88-b42e-00897bae084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_choices = cluster(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80f2e2",
   "metadata": {},
   "source": [
    "**The major pipeline components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a96e4672-3926-402d-b16c-85ec2caa98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet(fit_intercept=False, warm_start=True, random_state=0, selection='random', max_iter=4000)\n",
    "\n",
    "# Used for embedded feature importance (via coeffs) and wrapper feature importance (via perm importance)\n",
    "pipe_elastic_net = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"elastic_net\", elastic_net)\n",
    "    ],\n",
    "    memory = root+'\\\\cache',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Hyperparameters for elastic net tuning. When code is finalized, expand for more thorough search using more computational resources.\n",
    "REGULARIZATION = np.logspace(-5, 0, 8)\n",
    "MIXTURE = np.linspace(0.001, 1, 8)\n",
    "PARAM_GRID = [\n",
    "    {\n",
    "        \"elastic_net__alpha\": REGULARIZATION,\n",
    "        \"elastic_net__l1_ratio\": MIXTURE\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d64d3-904a-4f44-a923-cb8ce27dd59a",
   "metadata": {},
   "source": [
    "**Feature selection functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef09dedf-8f50-4c85-951d-c911328949bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi(X_train, y_train, n_features=64):\n",
    "    \"\"\" Uses mutual information to calculate the n_features most related features in X_train to y_train. (Filter method) \"\"\"\n",
    "    y_train = y_train.ravel()\n",
    "    mi = mutual_info_regression(X_train, y_train)\n",
    "    top_n_idx = np.argpartition(mi, -n_features)[-n_features:]\n",
    "    return wvs[top_n_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c05613-121e-4666-ba40-c285cf7c4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_elastic_net(X_train, y_train):\n",
    "    \"\"\" Builds and fits an elastic net model using all features. \n",
    "    Returns the fit estimator (a pipeline). Used within coeffs() and ga(). \"\"\"\n",
    "    grid = GridSearchCV(estimator=pipe_elastic_net, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "    grid.fit(X_train, y_train)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e5f4d2-3b6f-433f-92df-b2c8279ba4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeffs(estimator, n_features=64):\n",
    "    \"\"\" Builds and fits an elastic net model using all features. Returns the n_features features with the highest absolute-valued coefficients. (Embedded method) \"\"\"\n",
    "    coeffs = estimator['elastic_net'].coef_\n",
    "    abs_coeffs = np.abs(coeffs)\n",
    "    top_n_idx = np.argpartition(abs_coeffs, -n_features)[-n_features:]\n",
    "    return wvs[top_n_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "693601be-723e-4ce0-b1a5-676fbdd59413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ga(X_train, y_train, trained_estimator, n_features=64, wv_subset=None):\n",
    "    \"\"\" Uses a genetic algorithm to find the wavebands that gives the lowest RMSE on an elastic net model. \n",
    "    The subset will be at most n_features large, but it may be less than n_features large. \n",
    "    wv_subset should be None when in the feature selection layer, but when in the consolidation layer, it should\n",
    "    be the subset of possible wavelengths output by the concatenated feature selection methods, not the entire\n",
    "    [350,2500] set. (GA method) \"\"\"\n",
    "    \n",
    "    y_train = y_train.ravel()\n",
    "    ga_selector = GAFeatureSelectionCV(\n",
    "        estimator=trained_estimator,\n",
    "        cv=cv_5_0,  # Cross-validation folds\n",
    "        scoring=\"neg_root_mean_squared_error\",  # Fitness function (maximize accuracy)\n",
    "        population_size=n_features*2,  # Number of individuals in the population\n",
    "        generations=50,  # Number of generations\n",
    "        n_jobs=-1,  # Use all available CPU cores\n",
    "        verbose=False,\n",
    "        max_features=n_features,\n",
    "        return_train_score=True,\n",
    "        refit=False,\n",
    "        crossover_probability=0.8,\n",
    "        mutation_probability=0.2\n",
    "    )\n",
    "    pipe_ga = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"ga\", ga_selector)\n",
    "        ], \n",
    "        memory = root+'\\\\cache',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    pipe_ga.fit(X_train, y_train)\n",
    "    feats = pipe_ga['ga'].best_features_ # A mask of the features selected from X_train\n",
    "\n",
    "    # Should be the case in the feature selection layer\n",
    "    if wv_subset is None:\n",
    "        return wvs[feats]\n",
    "    # Should be the case in the consensus layer\n",
    "    else:\n",
    "        return wv_subset[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78bd175-1cf6-4533-a305-dacce062f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_imp(X_train, y_train, n_features=64):\n",
    "    \"\"\" Calculates permutation importance on a dataset. cluster_choices should be the result of calling cluster(), which should be done once at the start of execution. \n",
    "    This is done outside this function to preserve the random selection. Returns the set of n_features wavebands with the highest permutation importance on the training set. (Wrapper method) \"\"\"\n",
    "    # Use only the features selected by clustering\n",
    "    cluster_idx = cluster_choices - 350\n",
    "    X_train = X_train[:,cluster_idx]\n",
    "    # Build and train another elastic net model, but only on the features left after clustering, to use for permutation importance.\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"elastic_net\", elastic_net)\n",
    "        ], \n",
    "        memory = root+'\\\\cache',\n",
    "        verbose=False\n",
    "    )    \n",
    "    grid = GridSearchCV(estimator=pipe, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "    grid.fit(X_train, y_train)\n",
    "    perm_imp = permutation_importance(grid, X_train, y_train, scoring='neg_root_mean_squared_error', n_repeats=10, n_jobs=-1, random_state=0)\n",
    "    pi_top_64_idx = np.argpartition(perm_imp.importances_mean, -64)[-64:]\n",
    "    return cluster_choices[pi_top_64_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465453b6-7b18-40c6-b88a-502d1a9f4c9e",
   "metadata": {},
   "source": [
    "**Consensus function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29ed9401-293e-4afd-82e3-45e57f1bccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus(X_train, y_train, n_features_intermed=64, max_features_output=8):\n",
    "    \"\"\" Takes the wavebands output by the feature selection functions and uses a (separate) genetic algorithm to find the wavebands that give the lowest RMSE on an elastic net model.\n",
    "    The subset will be at most n_features large, but it may be less than n_features large.\n",
    "    Returns the tuple: (wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus), where each is a numpy array of wavebands that were selected by each method.  (Consensus method) \"\"\"\n",
    "    \n",
    "    print('\\tFEATURE SELECTION:')\n",
    "    print('\\tStarting mutual importance...', end=' ')\n",
    "    wv_mi = mi(X_train, y_train, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "    print('\\tTraining the elastic net model...', end=' ')\n",
    "    trained_pipe = train_elastic_net(X_train, y_train)\n",
    "    wv_coeffs = coeffs(trained_pipe, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "    print('\\tStarting genetic algorithm...', end=' ')\n",
    "    wv_ga = ga(X_train, y_train, trained_pipe, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "    print('\\tStarting permutation importance...', end=' ')\n",
    "    wv_cluster = cluster_choices # Doesn't require a separate function call\n",
    "    wv_perm_imp = perm_imp(X_train, y_train, n_features=n_features_intermed)\n",
    "    print('Done.')\n",
    "\n",
    "    # Compile the above results into one array, remove any duplicates, and sort.\n",
    "    wv_intermed = np.append(wv_mi, wv_coeffs)\n",
    "    wv_intermed = np.append(wv_intermed, wv_ga)\n",
    "    wv_intermed = np.append(wv_intermed, wv_cluster)\n",
    "    wv_intermed = np.append(wv_intermed, wv_perm_imp)\n",
    "    wv_intermed = np.sort(np.unique(wv_intermed))\n",
    "\n",
    "    # Convert the above into indices for masking over the dataset.\n",
    "    wv_intermed_idx = wv_intermed-350\n",
    "    X_train = X_train[:,wv_intermed_idx]\n",
    "\n",
    "    # Use another genetic algorithm to find the best wavebands out of the narrowed possibilities\n",
    "    print('\\tCONSENSUS:')\n",
    "    print('\\tStarting genetic algorithm...', end=' ')\n",
    "    wv_consensus = ga(X_train, y_train, trained_pipe, n_features=max_features_output, wv_subset=wv_intermed)\n",
    "    print('\\tDone.')\n",
    "    return (wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a49f65f-065b-47fc-b4f1-4eeb463189df",
   "metadata": {},
   "source": [
    "**The \"main\" function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d5e1a93-cf0d-4df9-9166-deb913ef0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would normally be in the main function, but defined separately for easier testing, debugging, and analysis after running within a Jupyter notebook.\n",
    "def run():\n",
    "    # Lists of results that will be compiled at the end into a DataFrame for writing to CSV\n",
    "    gene_list = []\n",
    "    method_list = []\n",
    "    wv_list = []\n",
    "    rmse_list = []\n",
    "\n",
    "    # Loop over each gene (y value)\n",
    "    for gene, y_train, y_test in zip((\"phoa\", \"cbblr\", \"fungi\", \"bact\", \"urec\"), (phoa_train, cbblr_train, fungi_train, bact_train, urec_train), (phoa_test, cbblr_test, fungi_test, bact_test, urec_test)):\n",
    "\n",
    "        print(\"Starting \", gene, \"...\", sep = \"\")\n",
    "        \n",
    "        # Where the main calculations happen. Runs each method separately, and then finds the consensus of all of them.\n",
    "        # The phoa special case is due to a different train/test split than the rest because of some NANs.\n",
    "        if(gene == \"phoa\"):\n",
    "            wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus = consensus(X_phoa_train, y_train)\n",
    "        else:\n",
    "            wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus = consensus(X_train, y_train)\n",
    "        \n",
    "        for method, wv_set in zip((\"mi\", \"coeffs\", \"ga\", \"cluster\", \"perm_imp\"), (wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus)):\n",
    "            \n",
    "            # Build a new elastic net model for validation on this subset of wavebands\n",
    "            wv_set_idx = wv_set-350\n",
    "            validator = GridSearchCV(estimator=pipe_elastic_net, param_grid=PARAM_GRID, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=cv_5_0, error_score='raise')\n",
    "            # Like above, special case for phoa\n",
    "            if(gene == \"phoa\"):\n",
    "                validator.fit(X_phoa_train[:,wv_set_idx], y_train)\n",
    "                rmse = validator.score(X_phoa_test[:,wv_set_idx], y_test)\n",
    "            else:\n",
    "                validator.fit(X_train[:,wv_set_idx], y_train)\n",
    "                rmse = validator.score(X_test[:,wv_set_idx], y_test)\n",
    "\n",
    "            # Record each waveband separately. This is difficult to analyze visually, but makes for MUCH easier analysis in R later.\n",
    "            for wv in wv_set:\n",
    "                gene_list.append(gene)\n",
    "                method_list.append(method)\n",
    "                wv_list.append(wv)\n",
    "                rmse_list.append(rmse)\n",
    "                \n",
    "        print(\"Finished \", gene, \".\", sep = \"\")\n",
    "\n",
    "    # Compile the results into a single DataFrame and write it to a CSV\n",
    "    # The format will make for easier analysis later.\n",
    "    print(\"Compiling and writing results to CSV...\", end = \" \")\n",
    "    col_names = ['gene', 'method', 'wv', 'rmse']\n",
    "    results = pd.DataFrame(columns = col_names)\n",
    "    results['gene'] = gene_list\n",
    "    results['method'] = method_list\n",
    "    results['wv'] = wv_list\n",
    "    results['rmse'] = rmse_list\n",
    "    results.to_csv(root + '//results/results.csv', index=False)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d70ab9f9-5fe6-4e67-a177-e48025b4106b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting phoa...\n",
      "\tFEATURE SELECTION:\n",
      "\tStarting mutual importance... Done.\n",
      "\tTraining the elastic net model... Done.\n",
      "\tStarting genetic algorithm... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn_genetic\\algorithms.py:211: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  \"\"\"\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn_genetic\\algorithms.py:396: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not pickle the task to send it to the workers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 159, in _feed\n    obj_ = dumps(obj, reducers=reducers)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 215, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 208, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle.py\", line 1245, in dump\n    return super().dump(obj)\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\joblib\\_memmapping_reducer.py\", line 401, in __call__\n    raise e\n  File \"C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\joblib\\_memmapping_reducer.py\", line 397, in __call__\n    os.makedirs(self._temp_folder)\n  File \"C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\os.py\", line 225, in makedirs\n    mkdir(name, mode)\nOSError: [WinError 112] There is not enough space on the disk: 'C:\\\\Users\\\\JOSHUA~1.WAL\\\\AppData\\\\Local\\\\Temp\\\\joblib_memmapping_folder_22276_003cea42072b4f7c95b321f65583fcc7_bb3f7ac750204e65a4d22962d4cdef6f'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m run()\n",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m, in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Where the main calculations happen. Runs each method separately, and then finds the consensus of all of them.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# The phoa special case is due to a different train/test split than the rest because of some NANs.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(gene \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphoa\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus \u001b[38;5;241m=\u001b[39m consensus(X_phoa_train, y_train)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     wv_mi, wv_coeffs, wv_ga, wv_cluster, wv_perm_imp, wv_consensus \u001b[38;5;241m=\u001b[39m consensus(X_train, y_train)\n",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m, in \u001b[0;36mconsensus\u001b[1;34m(X_train, y_train, n_features_intermed, max_features_output)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mStarting genetic algorithm...\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m wv_ga \u001b[38;5;241m=\u001b[39m ga(X_train, y_train, trained_pipe, n_features\u001b[38;5;241m=\u001b[39mn_features_intermed)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mStarting permutation importance...\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m, in \u001b[0;36mga\u001b[1;34m(X_train, y_train, trained_estimator, n_features, wv_subset)\u001b[0m\n\u001b[0;32m      9\u001b[0m ga_selector \u001b[38;5;241m=\u001b[39m GAFeatureSelectionCV(\n\u001b[0;32m     10\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mtrained_estimator,\n\u001b[0;32m     11\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv_5_0,  \u001b[38;5;66;03m# Cross-validation folds\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     mutation_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m pipe_ga \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[0;32m     24\u001b[0m     [\n\u001b[0;32m     25\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m, StandardScaler()),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m pipe_ga\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     33\u001b[0m feats \u001b[38;5;241m=\u001b[39m pipe_ga[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mga\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbest_features_ \u001b[38;5;66;03m# A mask of the features selected from X_train\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Should be the case in the feature selection layer\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn_genetic\\genetic_search.py:1155\u001b[0m, in \u001b[0;36mGAFeatureSelectionCV.fit\u001b[1;34m(self, X, y, callbacks)\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register()\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;66;03m# Optimization routine from the selected evolutionary algorithm\u001b[39;00m\n\u001b[1;32m-> 1155\u001b[0m pop, log, n_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_algorithm(pop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pop, stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stats, hof\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hof)\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;66;03m# Update the _n_iterations value as the algorithm could stop earlier due a callback\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_iterations \u001b[38;5;241m=\u001b[39m n_gen\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn_genetic\\genetic_search.py:1219\u001b[0m, in \u001b[0;36mGAFeatureSelectionCV._select_algorithm\u001b[1;34m(self, pop, stats, hof)\u001b[0m\n\u001b[0;32m   1217\u001b[0m selected_algorithm \u001b[38;5;241m=\u001b[39m algorithms_factory\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_algorithm:\n\u001b[1;32m-> 1219\u001b[0m     pop, log, gen \u001b[38;5;241m=\u001b[39m selected_algorithm(\n\u001b[0;32m   1220\u001b[0m         pop,\n\u001b[0;32m   1221\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbox,\n\u001b[0;32m   1222\u001b[0m         mu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation_size,\n\u001b[0;32m   1223\u001b[0m         lambda_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation_size,\n\u001b[0;32m   1224\u001b[0m         cxpb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrossover_adapter,\n\u001b[0;32m   1225\u001b[0m         stats\u001b[38;5;241m=\u001b[39mstats,\n\u001b[0;32m   1226\u001b[0m         mutpb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmutation_adapter,\n\u001b[0;32m   1227\u001b[0m         ngen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerations,\n\u001b[0;32m   1228\u001b[0m         halloffame\u001b[38;5;241m=\u001b[39mhof,\n\u001b[0;32m   1229\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   1230\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   1231\u001b[0m         estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1232\u001b[0m     )\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1236\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe algorithm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1237\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease select one from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAlgorithms\u001b[38;5;241m.\u001b[39mlist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1238\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn_genetic\\algorithms.py:327\u001b[0m, in \u001b[0;36meaMuPlusLambda\u001b[1;34m(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, stats, halloffame, callbacks, verbose, estimator, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m invalid_ind \u001b[38;5;241m=\u001b[39m [ind \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m offspring \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ind\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalid]\n\u001b[0;32m    326\u001b[0m fitnesses \u001b[38;5;241m=\u001b[39m toolbox\u001b[38;5;241m.\u001b[39mmap(toolbox\u001b[38;5;241m.\u001b[39mevaluate, invalid_ind)\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, fit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(invalid_ind, fitnesses):\n\u001b[0;32m    328\u001b[0m     ind\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m fit\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# Update the hall of fame with the generated individuals\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn_genetic\\genetic_search.py:1042\u001b[0m, in \u001b[0;36mGAFeatureSelectionCV.evaluate\u001b[1;34m(self, individual)\u001b[0m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfitness\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;66;03m# Compute the cv-metrics using only the selected features\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m   1043\u001b[0m     local_estimator,\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_[:, bool_individual],\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_,\n\u001b[0;32m   1046\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv,\n\u001b[0;32m   1047\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring,\n\u001b[0;32m   1048\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m   1049\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_dispatch,\n\u001b[0;32m   1050\u001b[0m     error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_score,\n\u001b[0;32m   1051\u001b[0m     return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_train_score,\n\u001b[0;32m   1052\u001b[0m )\n\u001b[0;32m   1054\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m cv_results[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefit_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1055\u001b[0m score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(cv_scores)\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    424\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    425\u001b[0m         clone(estimator),\n\u001b[0;32m    426\u001b[0m         X,\n\u001b[0;32m    427\u001b[0m         y,\n\u001b[0;32m    428\u001b[0m         scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[0;32m    429\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    430\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    431\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    432\u001b[0m         parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    433\u001b[0m         fit_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[0;32m    434\u001b[0m         score_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mscore,\n\u001b[0;32m    435\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    436\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    437\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    438\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    439\u001b[0m     )\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    441\u001b[0m )\n\u001b[0;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_error_fast()\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     error_job\u001b[38;5;241m.\u001b[39mget_result(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_raise()\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not pickle the task to send it to the workers."
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
