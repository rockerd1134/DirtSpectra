{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1c8e74-659c-4624-926c-12fcc27895b2",
   "metadata": {},
   "source": [
    "Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd91d8c-be40-4c4f-b55a-2aa13f2708f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "# Add more imports in this block later. There will need to be several \"from sklearn.whatever import something\" lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4168c99b-3a35-4dd1-a31c-f4daaae1880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = git.Repo('.', search_parent_directories = True)\n",
    "root = repo.working_tree_dir\n",
    "\n",
    "# The sample id and the log-transformed gene expression values.\n",
    "half_data_1 = pd.read_csv(root + '\\\\data\\\\RKNGHStress.csv')\n",
    "half_data_1 = half_data_1.loc[:, half_data_1.columns.str.startswith(('Sample', 'Log'))]\n",
    "half_data_1 = half_data_1.rename(columns = {'Sample' : 'sample', 'Log16S' : 'bact', 'Logcbblr' : 'cbblr', 'Log18S' : 'fungi', 'Logphoa' : 'phoa', 'Logurec' : 'urec'})\n",
    "\n",
    "# The hyperspectral measurements for each sample\n",
    "half_data_2 = pd.read_csv(root + '\\\\data\\\\RKNGHStressPCAPSR.csv')\n",
    "half_data_2 = half_data_2.rename(columns = {'Unnamed: 0' : 'sample'})\n",
    "\n",
    "data = half_data_1.join(half_data_2.set_index('sample'), on = 'sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d2cc1-a33c-43bc-b1dd-72b2ed921d26",
   "metadata": {},
   "source": [
    "TEMP: testing manual construction of models with specific hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "590160bb-bbbd-4c7a-bbba-4b892c5ac65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['sample', 'bact', 'cbblr', 'fungi', 'phoa', 'urec'], axis = 1)\n",
    "bact = data[['bact']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389c2476-08f3-419f-8f01-9be461b04156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4792568340990584"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, bact_train, bact_test = train_test_split(X.to_numpy(), bact.to_numpy(), train_size = 0.8, random_state = 0)\n",
    "bact_train = scale(bact_train.ravel())\n",
    "bact_test = scale(bact_test.ravel())\n",
    "\n",
    "# Note: do NOT scale X and y before splitting, since that is a data leak. Instead, use the pipeline to scale both Xs and the y training, and manually scale the y testing for custom scoring like RMSE.\n",
    "\n",
    "# For the sake of robustness, maybe should repeat this a few times, with different random states (still manually set for sake of reproducibility) e.g., 0, 1, ... , 4\n",
    "cv_0 = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "\n",
    "# n_jobs will need to be adjusted later when running on SCINet (high performance computing clusters). -1 uses all available cores, which might cause a bit of thrashing, but good enough for now.\n",
    "pipeline = make_pipeline(StandardScaler(), ElasticNetCV(alphas = [0.001389495], l1_ratio = 0.4285714, cv = cv_0, selection = 'random', max_iter = 10000, n_jobs = -1))\n",
    "pipeline.fit(X_train, bact_train)\n",
    "pipeline.score(X_test, bact_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f46efa-9605-446e-bc1d-91afb144bbe6",
   "metadata": {},
   "source": [
    "This looks weird. None of the folds are converging (true for 10 folds, 5 folds, or even just 1 fold with basic ElasticNet) but changing selection to 'random' and max_iter to 10k allowed convergence. \n",
    "\n",
    "The score is pretty low. But what metric is the score? If it's not RMSE it's not in the ballpark of the R version. (UPDATE: It's R^2, so it's a bunch of pretty bad scores, actually.)\n",
    "\n",
    "It looks like ElasticNetCV, and ElasticNet for that matter, don't allow changing the scoring or tuning metric, or at least it's not obvious how. Can the models be evaluated on RMSE just by calling the RMSE function on the preds and targets? Also, does it even make sense to change the tuning metric for this algorithm? (Idk, you probably COULD, but minimizing the sum of squared residuals is good enough.) And what tuning metric does the tidymodels implementation use?\n",
    "\n",
    "This all might be caused by using the hyperparameter optima found in the R code, but that had a differet train/test split. So it's not optimal here, but it's still probably pretty good, especially since in the analysis of part 1's hyperparameters, there wasn't much variation among the elastic net models' penalties (all tending to be very close to 0) or mixtures (a bit more spread but similar).\n",
    "\n",
    "What happens if some of its own tuning were allowed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c828750-f67b-460c-b8d8-94fd1b31a5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE\n",
      "0.7216253639534443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print('Preds on X_test')\n",
    "# print(pipeline.predict(X_test))\n",
    "# print()\n",
    "# print('Scaled bact_test')\n",
    "# print(scale(bact_test))\n",
    "# print()\n",
    "\n",
    "# print('MSE')\n",
    "# mse = mean_squared_error(scale(bact_test), pipeline.predict(X_test))\n",
    "# print(mse)\n",
    "\n",
    "print('RMSE')\n",
    "rmse = root_mean_squared_error(scale(bact_test), pipeline.predict(X_test))\n",
    "print(rmse)\n",
    "print()\n",
    "\n",
    "# print('MSE, no scaling bact_test')\n",
    "# mse1 = mean_squared_error(bact_test, pipeline.predict(X_test))\n",
    "# print(mse1)\n",
    "# print()\n",
    "\n",
    "# print('RMSE, no scaling bact_test')\n",
    "# rmse1 = root_mean_squared_error(bact_test, pipeline.predict(X_test))\n",
    "# print(rmse1)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23386e86-b543-4d9a-943f-dd7f622db698",
   "metadata": {},
   "source": [
    "Uh oh, RMSE of 0.83 is really bad. It doesn't even come close to the R models. What happened?\n",
    "\n",
    "Looking at the results, it looks like the scaling isn't happening for some reason, or at least not for the predictions. They're all in the 9 range instead of 0 range. Recalculating RMSE after removing scaling on bact_test gave more reasonable results.\n",
    "\n",
    "But the problem is, we want to be able to compare models among different targets using a common scale, so normalization has to be done with respect to other targets (but NOT the entire dataset for each column since that's data leakage). This should be done before training. But how can this be implemented? (UPDATE: Fixed by manually scaling bact_train and bact_test, separately. But still doesn't solve the issue of getting much higher RMSE than expected.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e983cfd9-22e6-48d4-930d-34a52c60f7a4",
   "metadata": {},
   "source": [
    "Decided to go back and change the cross validation size to 5 instead of 10 in light of the relatively small dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e9ad6d-89e8-4a40-ba49-c661efb9a91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.280e+02, tolerance: 2.560e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.326e+02, tolerance: 2.651e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.274e+02, tolerance: 2.548e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.289e+02, tolerance: 2.578e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.226e+02, tolerance: 2.452e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.325e+02, tolerance: 2.651e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.287e+02, tolerance: 2.578e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.279e+02, tolerance: 2.560e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.273e+02, tolerance: 2.548e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.224e+02, tolerance: 2.452e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.324e+02, tolerance: 2.651e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.276e+02, tolerance: 2.560e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.279e+02, tolerance: 2.578e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.217e+02, tolerance: 2.452e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.268e+02, tolerance: 2.548e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.323e+02, tolerance: 2.651e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.274e+02, tolerance: 2.560e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.212e+02, tolerance: 2.452e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.264e+02, tolerance: 2.548e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.275e+02, tolerance: 2.578e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\joshua.waldbieser\\.conda\\envs\\rkngh_stress\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.599e+02, tolerance: 3.200e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0003409445055663207"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing how RMSE changes (hopefully improves!) with hyperparameter tuning\n",
    "\n",
    "mix_space = np.linspace(0, 1, 8)\n",
    "reg_space = np.logspace(-5, 5, 8)\n",
    "\n",
    "elastcv = ElasticNetCV(alphas = reg_space, l1_ratio = mix_space, cv = cv_0, selection = 'random', max_iter = 10000, n_jobs = -1, random_state = 0, positive = True)\n",
    "estimators_cv = [('scaler', StandardScaler()), ('elastic_net', elastcv)]\n",
    "pipeline_hp = Pipeline(estimators_cv, memory = root + '\\\\cache')\n",
    "pipeline_hp.fit(X_train, bact_train)\n",
    "pipeline_hp.score(X_test, bact_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95fd1deb-7db8-4dbc-b03e-742ff24d7a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE\n",
      "0.999829513214345\n",
      "\n",
      "{'alphas': array([1.00000000e-05, 2.68269580e-04, 7.19685673e-03, 1.93069773e-01,\n",
      "       5.17947468e+00, 1.38949549e+02, 3.72759372e+03, 1.00000000e+05]), 'copy_X': True, 'cv': KFold(n_splits=5, random_state=0, shuffle=True), 'eps': 0.001, 'fit_intercept': True, 'l1_ratio': array([0.        , 0.14285714, 0.28571429, 0.42857143, 0.57142857,\n",
      "       0.71428571, 0.85714286, 1.        ]), 'max_iter': 10000, 'n_alphas': 100, 'n_jobs': -1, 'positive': True, 'precompute': 'auto', 'random_state': 0, 'selection': 'random', 'tol': 0.0001, 'verbose': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('RMSE')\n",
    "# bact_test has already been scaled above\n",
    "rmse_hp = root_mean_squared_error(bact_test, pipeline_hp.predict(X_test))\n",
    "print(rmse_hp)\n",
    "print()\n",
    "\n",
    "print(pipeline_hp['elastic_net'].get_params())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af218112-f933-4e6a-aca4-cf1ef8d73906",
   "metadata": {},
   "source": [
    "This sucks. The results are even worse than before. I must have done something wrong here, but I haven't figured out what yet. Also I'm trying to figure out what the hyperparameters were that it ended up on. Maybe it's better to just do ElasticNet and the parameter search separately in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a2624f3-da92-46db-9112-b27ae830faf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train, mean: [-9.08995101e-17 -1.38777878e-18 -1.23512311e-16 ...  4.37115621e-15\n",
      "  5.88210036e-15  1.26496036e-15]\n",
      "X_train, std: [1. 1. 1. ... 1. 1. 1.]\n",
      "X_test, mean: [ 4.71844785e-17  5.13478149e-17  2.13717932e-16 ...  2.18575158e-16\n",
      " -2.23432384e-15  1.82145965e-15]\n",
      "X_test, std: [1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Trying to get a sense of the spread of the data\n",
    "print('X_train, mean:', np.mean(scale(X_train), axis=0))\n",
    "print('X_train, std:', np.std(scale(X_train), axis=0))\n",
    "print('X_test, mean:', np.mean(scale(X_test), axis=0))\n",
    "print('X_test, std:', np.std(scale(X_test), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f935de72-ad11-4811-b1db-2f8f052327e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bact_train, mean: -9.117706589734098e-16\n",
      "bact_train, std: 0.9999999999999999\n",
      "bact_test, mean: -3.985700658404312e-15\n",
      "bact_test, std: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "print('bact_train, mean:', np.mean(bact_train))\n",
    "print('bact_train, std:', np.std(bact_train))\n",
    "print('bact_test, mean:', np.mean(bact_test))\n",
    "print('bact_test, std:', np.std(bact_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc0646a-ef83-4588-bc88-0b4297ed3563",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_space = np.linspace(0, 1, 8)\n",
    "reg_space = np.logspace(-5, 5, 8)\n",
    "\n",
    "elast = ElasticNetCV(alphas = reg_space, l1_ratio = mix_space, cv = cv_0, selection = 'random', max_iter = 10000, n_jobs = -1, random_state = 0, positive = True)\n",
    "estimators_cv = [('scaler', StandardScaler()), ('elastic_net', elastcv)]\n",
    "pipeline_hp = Pipeline(estimators_cv, memory = root + '\\\\cache')\n",
    "pipeline_hp.fit(X_train, bact_train)\n",
    "pipeline_hp.score(X_test, bact_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
